{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic(img, mosaic_rate):\n",
    "    w = img.shape[1]\n",
    "    h = img.shape[0]\n",
    "    img = cv2.resize(img, (int(w * mosaic_rate), int(h * mosaic_rate)))\n",
    "    img = cv2.resize(img, (w, h), interpolation = cv2.INTER_NEAREST)\n",
    "    return img\n",
    "\n",
    "cascade_file= cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "clas = cv2.CascadeClassifier(cascade_file)\n",
    "cap = cv2.VideoCapture(\"output.mp4\")\n",
    "\n",
    "#動画を保存\n",
    "fmt = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "fps = 20.0\n",
    "size = (640, 360)\n",
    "writer = cv2.VideoWriter('./test.mp4', fmt, fps, size)\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    #フレームのサイズを保存するサイズに合わせる\n",
    "    frame=cv2.resize(frame, size)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    face_list = clas.detectMultiScale(gray, scaleFactor = 1.1, minNeighbors = 3, minSize=(30,30))\n",
    "\n",
    "    for x, y, w, h in face_list:\n",
    "        frame[y : y+h, x : x+w] = mosaic(frame[y : y + h, x : x + w], 0.05)\n",
    "\n",
    "    #１フレームずつ画像を書き込む\n",
    "    writer.write(frame)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "writer.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YuNetのモデルファイルをダウンロード\n",
    "(https://github.com/opencv/opencv_zoo/blob/main/models/face_detection_yunet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フレームの幅: 1920\n",
      "フレームの高さ: 1080\n",
      "FPS: 59.94005994005994\n"
     ]
    }
   ],
   "source": [
    "# 動画ファイルを開く\n",
    "cap = cv2.VideoCapture(\"output2.mp4\")\n",
    "\n",
    "# 動画ファイルの各種プロパティーを取得\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # フレームの幅\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # フレームの高さ\n",
    "fps = float(cap.get(cv2.CAP_PROP_FPS))  # FPS\n",
    "\n",
    "print(f\"フレームの幅: {frame_width}\")\n",
    "print(f\"フレームの高さ: {frame_height}\")\n",
    "print(f\"FPS: {fps}\")\n",
    "\n",
    "# YuNet顔検出モデルの読み込み\n",
    "#yunet = cv2.FaceDetectorYN.create(\"face_detection_yunet_2023mar.onnx\",\n",
    "#                                  \"\", (frame_width, frame_height))\n",
    "\n",
    "\n",
    "yunet = cv2.FaceDetectorYN.create(model=\"face_detection_yunet_2023mar_int8bq.onnx\",\n",
    "                                  config=\"\",\n",
    "                                  input_size=(frame_width, frame_height))\n",
    "\n",
    "# VideoWriter を作成する。\n",
    "output_file = \"yu_net_output.mp4\"  # 保存する動画ファイル名\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 動画のコーデックを指定\n",
    "out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height), True)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 顔検出\n",
    "    faces = yunet.detect(frame)\n",
    "\n",
    "    # 顔の輪郭と目、鼻、口の位置を描画\n",
    "    if faces[1] is not None:\n",
    "        for idx, face in enumerate(faces[1]):\n",
    "            coords = face[:-1].astype(np.int32)\n",
    "            cv2.rectangle(frame, (coords[0], coords[1]), (coords[0]+coords[2], coords[1]+coords[3]), (0, 0, 255), 2)\n",
    "            cv2.circle(frame, (coords[4], coords[5]), 2, (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (coords[6], coords[7]), 2, (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (coords[8], coords[9]), 2, (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (coords[10], coords[11]), 2, (0, 255, 0), 2)\n",
    "            cv2.circle(frame, (coords[12], coords[13]), 2, (0, 255, 0), 2)\n",
    "\n",
    "    # 動画にフレームを書き込む\n",
    "    out.write(frame)\n",
    "\n",
    "# リソースを開放\n",
    "cap.release()\n",
    "out.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memo\n",
    "- 映像の画質が良くない\n",
    "- 顔認証の制度が良くない\n",
    "- 映像が反転している\n",
    "- 1分の動画に3分処理時間がかかる（量子化を試したい）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで高速化を試みる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yolov8n-face.ptを以下からDL\n",
    "https://github.com/akanametov/yolo-face?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOv8の顔検出モデル（事前に適切な顔検出モデルをダウンロード）\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device)  # 顔検出用のYOLOモデル\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = \"input.mp4\"\n",
    "output_video = \"output.mp4\"\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "def apply_mosaic(image, x1, y1, x2, y2, mosaic_scale=0.1):\n",
    "    \"\"\"指定した座標の範囲にモザイクを適用\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # 縮小して拡大することでモザイク効果を作成\n",
    "    small = cv2.resize(face, (max(1, int(w * mosaic_scale)), max(1, int(h * mosaic_scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    image[y1:y2, x1:x2] = mosaic\n",
    "    return image\n",
    "\n",
    "# フレーム処理\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 顔検出\n",
    "    results = model(frame)\n",
    "\n",
    "    # 各検出結果に対して処理\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "            frame = apply_mosaic(frame, x1, y1, x2, y2, mosaic_scale=0.05)\n",
    "\n",
    "    # 出力動画に書き込み\n",
    "    out.write(frame)\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"動画処理が完了しました。output.mp4 に保存されました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yolov8n-face.ptを以下からDL\n",
    "https://github.com/akanametov/yolo-face?tab=readme-ov-file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
