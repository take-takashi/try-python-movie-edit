{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from ultralytics import YOLO\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# .envファイルを読み込む\n",
    "load_dotenv()\n",
    "\n",
    "INPUT_VIDEO_PATH = os.getenv(\"INPUT_VIDEO_PATH\", \"default_input.mp4\")\n",
    "OUTPUT_VIDEO_PATH = INPUT_VIDEO_PATH + \"_edit.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(INPUT_VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.setNumThreads(0)  # OpenCVのスレッド管理を最適化\n",
    "\n",
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOモデルの読み込み（FP16でメモリ最適化）\n",
    "model = YOLO(\"yolov8s-face-lindevs.pt\").to(device).half()\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "processed_video = OUTPUT_VIDEO_PATH  # 音声なし映像\n",
    "output_video = OUTPUT_VIDEO_PATH + \"_c.mp4\"  # 最終的な音声付き動画\n",
    "\n",
    "# 動画の読み込み（ハードウェアアクセラレーションを有効化）\n",
    "cap = cv2.VideoCapture(input_video, cv2.CAP_FFMPEG)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定\n",
    "fourcc = cv2.VideoWriter_fourcc(*'h264')  # H.264 コーデック\n",
    "out = cv2.VideoWriter(processed_video, fourcc, fps, (width, height))\n",
    "\n",
    "batch_size = 20  # バッチサイズを拡張\n",
    "frames = []\n",
    "previous_faces = []\n",
    "frame_count = 0\n",
    "detection_interval = 5  # 5フレームごとに顔を検出\n",
    "\n",
    "def frosted_glass_blur(image, x1, y1, x2, y2, ksize=15):\n",
    "    \"\"\"すりガラス風のぼかし処理（Gaussian Blur）\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    blurred_face = cv2.GaussianBlur(face, (ksize, ksize), 0)\n",
    "    np.copyto(image[y1:y2, x1:x2], blurred_face)\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "def process_frames(frames):\n",
    "    \"\"\"フレームを並列処理する関数\"\"\"\n",
    "    results = model(frames, stream=True, verbose=False)\n",
    "    face_locations = []\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        faces = []\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            faces.append((x1, y1, x2, y2))\n",
    "            frosted_glass_blur(frames[i], x1, y1, x2, y2, ksize=21)  # ぼかし強度調整\n",
    "        face_locations.append(faces)\n",
    "        out.write(frames[i])\n",
    "\n",
    "    return face_locations\n",
    "\n",
    "# フレーム処理ループ\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frames.append(frame)\n",
    "\n",
    "    if frame_count % detection_interval == 0:\n",
    "        future = executor.submit(process_frames, frames)\n",
    "        previous_faces = future.result()\n",
    "    else:\n",
    "        for faces in previous_faces:\n",
    "            for x1, y1, x2, y2 in faces:\n",
    "                frosted_glass_blur(frames[0], x1, y1, x2, y2, ksize=21)\n",
    "        out.write(frames[0])\n",
    "\n",
    "    frames = []\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"映像処理が完了しました: {processed_video}\")\n",
    "\n",
    "# **音声をffmpegで合成**\n",
    "subprocess.run([\"ffmpeg\", \"-i\", processed_video, \"-i\", input_video, \"-c:v\", \"copy\", \"-c:a\", \"aac\", \"-strict\", \"experimental\", output_video], check=True)\n",
    "\n",
    "print(f\"音声付きの最終動画が作成されました: {output_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from ultralytics import YOLO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "cv2.setNumThreads(0)  # OpenCVのスレッド管理を最適化\n",
    "\n",
    "device = torch.device(\"mps\")  # M3 MacのGPUを使用\n",
    "model = YOLO(\"yolov8s-face-lindevs.pt\").to(device).half()\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "processed_video = OUTPUT_VIDEO_PATH  # 音声なし映像\n",
    "output_video = OUTPUT_VIDEO_PATH + \"_c.mp4\"  # 最終的な音声付き動画\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video, cv2.CAP_FFMPEG)\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'h264')\n",
    "out = cv2.VideoWriter(processed_video, fourcc, fps, (width, height))\n",
    "\n",
    "batch_size = 20  # バッチサイズ\n",
    "frames = []\n",
    "previous_faces = []\n",
    "frame_count = 0\n",
    "\n",
    "detection_interval = 5  # 5フレームごとに顔検出\n",
    "\n",
    "def frosted_glass_blur(image, x1, y1, x2, y2, ksize=25, padding=10):\n",
    "    \"\"\" すりガラス風のガウスぼかし処理 (顔の範囲を広めに適用) \"\"\"\n",
    "    x1, y1 = max(0, x1 - padding), max(0, y1 - padding)\n",
    "    x2, y2 = min(image.shape[1], x2 + padding), min(image.shape[0], y2 + padding)\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    blurred_face = cv2.GaussianBlur(face, (ksize, ksize), 0)\n",
    "    np.copyto(image[y1:y2, x1:x2], blurred_face)\n",
    "\n",
    "def process_frames(frames):\n",
    "    \"\"\" フレームを並列処理する関数 \"\"\"\n",
    "    with torch.no_grad():\n",
    "        results = model(frames, stream=True, verbose=False)\n",
    "    \n",
    "    face_locations = []\n",
    "    for i, result in enumerate(results):\n",
    "        faces = []\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            faces.append((x1, y1, x2, y2))\n",
    "            frosted_glass_blur(frames[i], x1, y1, x2, y2, ksize=25, padding=10)\n",
    "        face_locations.append(faces)\n",
    "        out.write(frames[i])\n",
    "    return face_locations\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frames.append(frame)\n",
    "    \n",
    "    if frame_count % detection_interval == 0:\n",
    "        future = executor.submit(process_frames, frames)\n",
    "        previous_faces = future.result()\n",
    "    else:\n",
    "        for faces in previous_faces:\n",
    "            for x1, y1, x2, y2 in faces:\n",
    "                frosted_glass_blur(frames[0], x1, y1, x2, y2, ksize=25, padding=10)\n",
    "        out.write(frames[0])\n",
    "    \n",
    "    frames = []\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "# **音声をffmpegで合成**\n",
    "subprocess.run([\"ffmpeg\", \"-i\", processed_video, \"-i\", input_video, \"-c:v\", \"copy\", \"-c:a\", \"aac\", \"-strict\", \"experimental\", output_video], check=True)\n",
    "\n",
    "print(f\"音声付きの最終動画が作成されました: {output_video}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ffmpeg使わない版にしたい\n",
    "GPTにもう一回動画をUPLOADして試してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from ultralytics import YOLO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "cv2.setNumThreads(0)  # OpenCVのスレッド管理を最適化\n",
    "\n",
    "device = torch.device(\"mps\")  # M3 MacのGPUを使用\n",
    "model = YOLO(\"yolov8s-face-lindevs.pt\").to(device).half()\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "processed_video = OUTPUT_VIDEO_PATH  # 音声なし映像\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video, cv2.CAP_FFMPEG)\n",
    "cap.set(cv2.CAP_PROP_BUFFERSIZE, 2)\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'h264')\n",
    "out = cv2.VideoWriter(processed_video, fourcc, fps, (width, height))\n",
    "\n",
    "batch_size = 20  # バッチサイズ\n",
    "frames = []\n",
    "previous_faces = []\n",
    "frame_count = 0\n",
    "\n",
    "detection_interval = 5  # 5フレームごとに顔検出\n",
    "\n",
    "def frosted_glass_blur(image, x1, y1, x2, y2, ksize=25, padding=10):\n",
    "    \"\"\" すりガラス風のガウスぼかし処理 (顔の範囲を広めに適用) \"\"\"\n",
    "    x1, y1 = max(0, x1 - padding), max(0, y1 - padding)\n",
    "    x2, y2 = min(image.shape[1], x2 + padding), min(image.shape[0], y2 + padding)\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    blurred_face = cv2.GaussianBlur(face, (ksize, ksize), 0)\n",
    "    np.copyto(image[y1:y2, x1:x2], blurred_face)\n",
    "\n",
    "def process_frames(frames):\n",
    "    \"\"\" フレームを並列処理する関数 \"\"\"\n",
    "    with torch.no_grad():\n",
    "        results = model(frames, stream=True, verbose=False)\n",
    "    \n",
    "    face_locations = []\n",
    "    for i, result in enumerate(results):\n",
    "        faces = []\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            faces.append((x1, y1, x2, y2))\n",
    "            frosted_glass_blur(frames[i], x1, y1, x2, y2, ksize=25, padding=10)\n",
    "        face_locations.append(faces)\n",
    "        out.write(frames[i])\n",
    "    return face_locations\n",
    "\n",
    "executor = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frames.append(frame)\n",
    "    \n",
    "    if frame_count % detection_interval == 0:\n",
    "        future = executor.submit(process_frames, frames)\n",
    "        previous_faces = future.result()\n",
    "    else:\n",
    "        for faces in previous_faces:\n",
    "            for x1, y1, x2, y2 in faces:\n",
    "                frosted_glass_blur(frames[0], x1, y1, x2, y2, ksize=25, padding=10)\n",
    "        out.write(frames[0])\n",
    "    \n",
    "    frames = []\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"映像処理が完了しました: {processed_video}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "cv2.setNumThreads(0)  # OpenCVのスレッド管理を最適化\n",
    "\n",
    "device = torch.device(\"mps\")  # M3 MacのGPUを使用\n",
    "model = YOLO(\"yolov8s-face-lindevs.pt\").to(device).half()\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "processed_video = OUTPUT_VIDEO_PATH  # 音声なし映像\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video, cv2.CAP_FFMPEG)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'h264')\n",
    "out = cv2.VideoWriter(processed_video, fourcc, fps, (width, height))\n",
    "\n",
    "detection_interval = 1  # フレーム間隔を縮小\n",
    "padding = 20  # モザイク範囲を拡張\n",
    "batch_size = 16  # バッチサイズを調整\n",
    "\n",
    "def frosted_glass_blur(image, x1, y1, x2, y2, ksize=25, padding=20):\n",
    "    \"\"\" すりガラス風のガウスぼかし処理 \"\"\"\n",
    "    x1, y1 = max(0, x1 - padding), max(0, y1 - padding)\n",
    "    x2, y2 = min(image.shape[1], x2 + padding), min(image.shape[0], y2 + padding)\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    if face.size > 0:\n",
    "        blurred_face = cv2.GaussianBlur(face, (ksize, ksize), 0)\n",
    "        np.copyto(image[y1:y2, x1:x2], blurred_face)\n",
    "\n",
    "def process_frame(frame):\n",
    "    \"\"\" 単一フレームを処理する関数 \"\"\"\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        frosted_glass_blur(frame, x1, y1, x2, y2, ksize=25, padding=20)\n",
    "    return frame\n",
    "\n",
    "frame_count = 0\n",
    "executor = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 顔検出間隔を調整\n",
    "    if frame_count % detection_interval == 0:\n",
    "        frame = process_frame(frame)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_count += 1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"映像処理が完了しました: {processed_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモ：  \n",
    "3フレームごとに検出：02:50  \n",
    "毎フレームごとに検出：06:36　でもクオリティ高い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% poetry add deep-sort-realtime\n",
    "\n",
    "トラッキング機能を導入するため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# GPU設定\n",
    "device = \"mps\"  # Mac M3用\n",
    "model = YOLO(\"yolov8s-face-lindevs.pt\").to(device).half()\n",
    "\n",
    "# DeepSORTトラッカーの初期化\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=70)\n",
    "\n",
    "# 動画設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "output_video = OUTPUT_VIDEO_PATH\n",
    "\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'h264')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "# モザイク処理関数\n",
    "def apply_mosaic(image, x1, y1, x2, y2, pixel_size=15):\n",
    "    \"\"\" モザイク処理を適用 \"\"\"\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)\n",
    "    \n",
    "    if x1 >= x2 or y1 >= y2:  # 無効な領域のチェック\n",
    "        return\n",
    "\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    if face.size > 0:\n",
    "        face = cv2.resize(face, (pixel_size, pixel_size), interpolation=cv2.INTER_LINEAR)\n",
    "        face = cv2.resize(face, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "        image[y1:y2, x1:x2] = face\n",
    "\n",
    "# フレーム処理ループ\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOで顔検出\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    detections = []\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        score = float(box.conf[0])\n",
    "        if score > 0.5:  # 信頼度スコアがしきい値を超える場合\n",
    "            detections.append([(x1, y1, x2, y2), score])\n",
    "\n",
    "    # トラッキング\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    # モザイク処理\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed():\n",
    "            continue  # 確定していないトラックはスキップ\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, track.to_tlbr())  # bbox座標を取得\n",
    "        apply_mosaic(frame, x1, y1, x2, y2)  # モザイク適用\n",
    "\n",
    "    # フレームの書き込み\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"映像処理が完了しました: {output_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんと15:11もかかった上にモザイクはぜんぜん当てにならない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "# GPU設定\n",
    "device = \"mps\"  # Mac M3用\n",
    "model = YOLO(\"yolov8s-face-lindevs.pt\").to(device).half()\n",
    "\n",
    "# DeepSORTトラッカーの初期化\n",
    "tracker = DeepSort(max_age=30, n_init=3, nn_budget=70)\n",
    "\n",
    "# 動画設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "output_video = OUTPUT_VIDEO_PATH\n",
    "\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'h264')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "# モザイク処理関数\n",
    "def apply_mosaic(image, x1, y1, x2, y2, pixel_size=15):\n",
    "    \"\"\" モザイク処理を適用 \"\"\"\n",
    "    x1, y1 = max(0, x1), max(0, y1)\n",
    "    x2, y2 = min(image.shape[1], x2), min(image.shape[0], y2)\n",
    "    \n",
    "    if x1 >= x2 or y1 >= y2:  # 無効な領域のチェック\n",
    "        return\n",
    "\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    if face.size > 0:\n",
    "        face = cv2.resize(face, (pixel_size, pixel_size), interpolation=cv2.INTER_LINEAR)\n",
    "        face = cv2.resize(face, (x2 - x1, y2 - y1), interpolation=cv2.INTER_NEAREST)\n",
    "        image[y1:y2, x1:x2] = face\n",
    "\n",
    "# フレーム処理ループ\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOで顔検出\n",
    "    results = model(frame, verbose=False)[0]\n",
    "    detections = []\n",
    "    for box in results.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        score = float(box.conf[0])\n",
    "        if score > 0.5:  # 信頼度スコアがしきい値を超える場合\n",
    "            detections.append([(x1, y1, x2, y2), score])\n",
    "\n",
    "    # トラッキング\n",
    "    tracked_objects = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "    # モザイク処理\n",
    "    for track in tracked_objects:\n",
    "        if not track.is_confirmed() or track.time_since_update > 1:\n",
    "            continue  # 確定していないトラックや更新されていないトラックはスキップ\n",
    "\n",
    "        x1, y1, x2, y2 = map(int, track.to_tlbr())  # bbox座標を取得\n",
    "        apply_mosaic(frame, x1, y1, x2, y2)  # モザイク適用\n",
    "\n",
    "    # フレームの書き込み\n",
    "    out.write(frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"映像処理が完了しました: {output_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15:18かかる。割にダメ。トラッキングは一旦諦める。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
