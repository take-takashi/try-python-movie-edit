{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# .envファイルを読み込む\n",
    "load_dotenv()\n",
    "\n",
    "INPUT_VIDEO_PATH = os.getenv(\"INPUT_VIDEO_PATH\", \"default_input.mp4\")\n",
    "OUTPUT_VIDEO_PATH = os.getenv(\"OUTPUT_VIDEO_PATH\", \"processed_output.mp4\")  # 音声なし映像"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOv8の顔検出モデル（事前に適切な顔検出モデルをダウンロード）\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device)  # 顔検出用のYOLOモデル\n",
    "model.half()  # FP16モードで動作（M3 Macはこの方が早い？）\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "output_video = OUTPUT_VIDEO_PATH\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "def apply_mosaic(image, x1, y1, x2, y2, mosaic_scale=0.1):\n",
    "    \"\"\"指定した座標の範囲にモザイクを適用\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # 縮小して拡大することでモザイク効果を作成\n",
    "    small = cv2.resize(face, (max(1, int(w * mosaic_scale)), max(1, int(h * mosaic_scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    image[y1:y2, x1:x2] = mosaic\n",
    "    return image\n",
    "\n",
    "# フレーム処理\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 顔検出\n",
    "    results = model(frame)\n",
    "\n",
    "    # 各検出結果に対して処理\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "            frame = apply_mosaic(frame, x1, y1, x2, y2, mosaic_scale=0.05)\n",
    "\n",
    "    # 出力動画に書き込み\n",
    "    out.write(frame)\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"動画処理が完了しました。output.mp4 に保存されました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ログを出力するから遅い可能性はないか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "動画処理が完了しました。output.mp4 に保存されました。\n"
     ]
    }
   ],
   "source": [
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOv8の顔検出モデル（事前に適切な顔検出モデルをダウンロード）\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device)  # 顔検出用のYOLOモデル\n",
    "model.half()  # FP16モードで動作（M3 Macはこの方が早い？）\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "output_video = OUTPUT_VIDEO_PATH\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "def apply_mosaic(image, x1, y1, x2, y2, mosaic_scale=0.1):\n",
    "    \"\"\"指定した座標の範囲にモザイクを適用\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # 縮小して拡大することでモザイク効果を作成\n",
    "    small = cv2.resize(face, (max(1, int(w * mosaic_scale)), max(1, int(h * mosaic_scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    image[y1:y2, x1:x2] = mosaic\n",
    "    return image\n",
    "\n",
    "# フレーム処理\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 顔検出\n",
    "    #results = model(frame)\n",
    "    # ここでログを出力停止\n",
    "    results = model(frame, stream=True, verbose=False)\n",
    "\n",
    "    # 各検出結果に対して処理\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "            frame = apply_mosaic(frame, x1, y1, x2, y2, mosaic_scale=0.05)\n",
    "\n",
    "    # 出力動画に書き込み\n",
    "    out.write(frame)\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"動画処理が完了しました。output.mp4 に保存されました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02:28秒から02:06秒になったので少し早くなった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "動画処理が完了しました。output.mp4 に保存されました。\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOv8の顔検出モデルをロード\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device)\n",
    "model.half()  # FP16モードで動作（ここで挿入！）\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "output_video = OUTPUT_VIDEO_PATH\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "# バッチ処理設定（例: 5フレームずつまとめて処理）\n",
    "batch_size = 5\n",
    "frames = []\n",
    "\n",
    "def apply_mosaic(image, x1, y1, x2, y2, mosaic_scale=0.1):\n",
    "    \"\"\"指定した座標の範囲にモザイクを適用\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # 縮小して拡大することでモザイク効果を作成\n",
    "    small = cv2.resize(face, (max(1, int(w * mosaic_scale)), max(1, int(h * mosaic_scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    image[y1:y2, x1:x2] = mosaic\n",
    "    return image\n",
    "\n",
    "# フレーム処理\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frames.append(frame)\n",
    "\n",
    "    # バッチサイズに達したら処理\n",
    "    if len(frames) == batch_size:\n",
    "        results = model(frames, stream=True, verbose=False)  # バッチ推論\n",
    "        for i, result in enumerate(results):\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "                frames[i] = apply_mosaic(frames[i], x1, y1, x2, y2, mosaic_scale=0.05)\n",
    "            out.write(frames[i])\n",
    "        frames = []\n",
    "\n",
    "# 残りのフレーム処理\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"動画処理が完了しました。output.mp4 に保存されました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バッチでまとめて処理にすることで\n",
    "01:49秒まで短縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOv8の顔検出モデルをロード\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device)\n",
    "model.half()  # FP16モードで動作（ここで挿入！）\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "output_video = OUTPUT_VIDEO_PATH\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "# バッチ処理設定（例: 5フレームずつまとめて処理）\n",
    "batch_size = 5\n",
    "frames = []\n",
    "\n",
    "def apply_mosaic(image, x1, y1, x2, y2, mosaic_scale=0.1):\n",
    "    \"\"\"指定した座標の範囲にモザイクを適用\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # 縮小して拡大することでモザイク効果を作成\n",
    "    small = cv2.resize(face, (max(1, int(w * mosaic_scale)), max(1, int(h * mosaic_scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    image[y1:y2, x1:x2] = mosaic\n",
    "    return image\n",
    "\n",
    "# フレーム処理\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frames.append(frame)\n",
    "\n",
    "    # バッチサイズに達したら処理\n",
    "    if len(frames) == batch_size:\n",
    "        results = model(frames, stream=True, verbose=False)  # バッチ推論\n",
    "        for i, result in enumerate(results):\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "                frames[i] = apply_mosaic(frames[i], x1, y1, x2, y2, mosaic_scale=0.05)\n",
    "            out.write(frames[i])\n",
    "        frames = []\n",
    "\n",
    "# 残りのフレーム処理\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"動画処理が完了しました。output.mp4 に保存されました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに高速化を試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "映像処理が完了しました: /Users/takashi/Downloads/撮影テスト/output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import subprocess\n",
    "\n",
    "# OpenCVのマルチスレッド無効化（シングルスレッドの方が速いことが多い）\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOモデルの読み込み（FP16でメモリ最適化）\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device).half()\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "processed_video = OUTPUT_VIDEO_PATH  # 音声なし映像\n",
    "output_video = \"output.mp4\"  # 最終的な音声付き動画\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定（音声なし）\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(processed_video, fourcc, fps, (width, height))\n",
    "\n",
    "# YOLOのバッチ処理を拡張（10フレームずつ処理）\n",
    "batch_size = 10\n",
    "frames = []\n",
    "\n",
    "def fast_mosaic(image, x1, y1, x2, y2, scale=0.05):\n",
    "    \"\"\"NumPyベースの高速モザイク処理\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # NumPy + OpenCVを使ったモザイク処理\n",
    "    small = cv2.resize(face, (max(1, int(w * scale)), max(1, int(h * scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 高速コピー\n",
    "    np.copyto(image[y1:y2, x1:x2], mosaic)\n",
    "\n",
    "# フレーム処理ループ\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frames.append(frame)\n",
    "\n",
    "    # バッチ処理を実行\n",
    "    if len(frames) == batch_size:\n",
    "        results = model(frames, stream=True, verbose=False)  # バッチ推論\n",
    "        for i, result in enumerate(results):\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "                fast_mosaic(frames[i], x1, y1, x2, y2, scale=0.05)\n",
    "            out.write(frames[i])\n",
    "        frames = []\n",
    "\n",
    "# 残りのフレームを処理\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"映像処理が完了しました: {processed_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらなる高速化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "映像処理が完了しました: /Users/takashi/Downloads/撮影テスト/output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import subprocess\n",
    "\n",
    "# OpenCVのマルチスレッド無効化（シングルスレッドの方が速いことが多い）\n",
    "cv2.setNumThreads(0)\n",
    "\n",
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOモデルの読み込み（FP16でメモリ最適化）\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device).half()\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "processed_video = OUTPUT_VIDEO_PATH  # 音声なし映像\n",
    "output_video = \"output.mp4\"  # 最終的な音声付き動画\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定（音声なし）\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(processed_video, fourcc, fps, (width, height))\n",
    "\n",
    "# YOLOのバッチ処理を拡張（10フレームずつ処理）\n",
    "batch_size = 10\n",
    "frames = []\n",
    "\n",
    "# 直近の顔座標を保存する変数\n",
    "previous_faces = []\n",
    "frame_count = 0\n",
    "detection_interval = 5  # 5フレームごとにYOLOで検出\n",
    "\n",
    "def fast_mosaic(image, x1, y1, x2, y2, scale=0.05):\n",
    "    \"\"\"NumPyベースの高速モザイク処理\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # モザイク処理\n",
    "    small = cv2.resize(face, (max(1, int(w * scale)), max(1, int(h * scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 高速コピー\n",
    "    np.copyto(image[y1:y2, x1:x2], mosaic)\n",
    "\n",
    "# フレーム処理ループ\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frames.append(frame)\n",
    "\n",
    "    # 5フレームごとにYOLOで顔を検出\n",
    "    if frame_count % detection_interval == 0:\n",
    "        results = model(frames, stream=True, verbose=False)  # バッチ推論\n",
    "        previous_faces = []  # 前のフレームの顔情報をクリア\n",
    "        for i, result in enumerate(results):\n",
    "            faces = []\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "                faces.append((x1, y1, x2, y2))\n",
    "                fast_mosaic(frames[i], x1, y1, x2, y2, scale=0.05)\n",
    "            previous_faces.append(faces)\n",
    "            out.write(frames[i])\n",
    "    else:\n",
    "        # 前のフレームの顔座標をそのまま使用（YOLOの推論をスキップ）\n",
    "        for faces in previous_faces:\n",
    "            for x1, y1, x2, y2 in faces:\n",
    "                fast_mosaic(frames[0], x1, y1, x2, y2, scale=0.05)\n",
    "        out.write(frames[0])\n",
    "\n",
    "    frames = []\n",
    "    frame_count += 1\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"映像処理が完了しました: {processed_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シングルスレッドで動かして速度をチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "映像処理が完了しました: /Users/takashi/Downloads/撮影テスト/output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import subprocess\n",
    "\n",
    "# OpenCVのマルチスレッド無効化（シングルスレッドで動作）\n",
    "cv2.setNumThreads(1)\n",
    "\n",
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOモデルの読み込み（FP16でメモリ最適化）\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device).half()\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "processed_video = OUTPUT_VIDEO_PATH  # 音声なし映像\n",
    "output_video = \"output.mp4\"  # 最終的な音声付き動画\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定（音声なし）\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(processed_video, fourcc, fps, (width, height))\n",
    "\n",
    "# 直近の顔座標を保存する変数\n",
    "previous_faces = []\n",
    "frame_count = 0\n",
    "detection_interval = 5  # 5フレームごとにYOLOで検出\n",
    "\n",
    "def fast_mosaic(image, x1, y1, x2, y2, scale=0.05):\n",
    "    \"\"\"NumPyベースの高速モザイク処理\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # モザイク処理\n",
    "    small = cv2.resize(face, (max(1, int(w * scale)), max(1, int(h * scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 高速コピー\n",
    "    np.copyto(image[y1:y2, x1:x2], mosaic)\n",
    "\n",
    "# フレーム処理ループ（シングルスレッド版）\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 5フレームごとにYOLOで顔を検出\n",
    "    if frame_count % detection_interval == 0:\n",
    "        results = model(frame, verbose=False)  # シングルスレッドで1フレームずつ推論\n",
    "        previous_faces = []  # 前のフレームの顔情報をクリア\n",
    "        for result in results:\n",
    "            faces = []\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "                faces.append((x1, y1, x2, y2))\n",
    "                fast_mosaic(frame, x1, y1, x2, y2, scale=0.05)\n",
    "            previous_faces.append(faces)\n",
    "    else:\n",
    "        # 前のフレームの顔座標をそのまま使用（YOLOの推論をスキップ）\n",
    "        for faces in previous_faces:\n",
    "            for x1, y1, x2, y2 in faces:\n",
    "                fast_mosaic(frame, x1, y1, x2, y2, scale=0.05)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_count += 1\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"映像処理が完了しました: {processed_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シングルスレッド版にしたら01:31秒かかった\n",
    "\n",
    "さらに10フレーム飛ばしにしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "映像処理が完了しました: /Users/takashi/Downloads/撮影テスト/output.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import subprocess\n",
    "\n",
    "# OpenCVのマルチスレッド無効化（シングルスレッドで動作）\n",
    "cv2.setNumThreads(1)\n",
    "\n",
    "# M3 MacのGPUを使用\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# YOLOモデルの読み込み（FP16でメモリ最適化）\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device).half()\n",
    "\n",
    "# 入出力ファイル設定\n",
    "input_video = INPUT_VIDEO_PATH\n",
    "processed_video = OUTPUT_VIDEO_PATH  # 音声なし映像\n",
    "output_video = \"output.mp4\"  # 最終的な音声付き動画\n",
    "\n",
    "# 動画の読み込み\n",
    "cap = cv2.VideoCapture(input_video)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# 出力動画の設定（音声なし）\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(processed_video, fourcc, fps, (width, height))\n",
    "\n",
    "# 直近の顔座標を保存する変数\n",
    "previous_faces = []\n",
    "frame_count = 0\n",
    "detection_interval = 10  # 10フレームごとにYOLOで検出\n",
    "\n",
    "def fast_mosaic(image, x1, y1, x2, y2, scale=0.05):\n",
    "    \"\"\"NumPyベースの高速モザイク処理\"\"\"\n",
    "    face = image[y1:y2, x1:x2]\n",
    "    h, w = face.shape[:2]\n",
    "\n",
    "    # モザイク処理\n",
    "    small = cv2.resize(face, (max(1, int(w * scale)), max(1, int(h * scale))), interpolation=cv2.INTER_LINEAR)\n",
    "    mosaic = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 高速コピー\n",
    "    np.copyto(image[y1:y2, x1:x2], mosaic)\n",
    "\n",
    "# フレーム処理ループ（シングルスレッド版）\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 5フレームごとにYOLOで顔を検出\n",
    "    if frame_count % detection_interval == 0:\n",
    "        results = model(frame, verbose=False)  # シングルスレッドで1フレームずつ推論\n",
    "        previous_faces = []  # 前のフレームの顔情報をクリア\n",
    "        for result in results:\n",
    "            faces = []\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # 顔の座標\n",
    "                faces.append((x1, y1, x2, y2))\n",
    "                fast_mosaic(frame, x1, y1, x2, y2, scale=0.05)\n",
    "            previous_faces.append(faces)\n",
    "    else:\n",
    "        # 前のフレームの顔座標をそのまま使用（YOLOの推論をスキップ）\n",
    "        for faces in previous_faces:\n",
    "            for x1, y1, x2, y2 in faces:\n",
    "                fast_mosaic(frame, x1, y1, x2, y2, scale=0.05)\n",
    "\n",
    "    out.write(frame)\n",
    "    frame_count += 1\n",
    "\n",
    "# 後処理\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"映像処理が完了しました: {processed_video}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60フレーム同じ場所にモザイクを置き続けるようにしたら、01:17秒。\n",
    "\n",
    "そこまで早くなってないし、モザイクがさすがにずれていく。\n",
    "\n",
    "これではダメ。\n",
    "\n",
    "マルチスレッドにして、もう少し長い動画で試すか。\n",
    "\n",
    "あと音声を残したい・・・"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
