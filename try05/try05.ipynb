{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# YOLOv11の顔検出モデルをロード\n",
    "model = YOLO(\"yolov11n-face.pt\")  # 軽量な顔検出モデル\n",
    "\n",
    "def pixelate(image, scale=0.1):\n",
    "    \"\"\"\n",
    "    ピクセレート処理（モザイク処理）\n",
    "    :param image: 画像の一部（顔の領域）\n",
    "    :param scale: 縮小率（小さいほど強いモザイク）\n",
    "    :return: ピクセレート後の画像\n",
    "    \"\"\"\n",
    "    # 画像を縮小\n",
    "    small = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
    "    # 縮小した画像を元のサイズに拡大\n",
    "    pixelated = cv2.resize(small, image.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n",
    "    return pixelated\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    動画を処理し、顔をピクセレートする\n",
    "    :param input_path: 入力動画のパス\n",
    "    :param output_path: 出力動画のパス\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # 顔検出\n",
    "        results = model(frame, stream=True, verbose=False)\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                # 顔領域の抽出\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                if face.shape[0] > 0 and face.shape[1] > 0:\n",
    "                    # ピクセレート処理\n",
    "                    pixelated_face = pixelate(face, scale=0.05)  # 0.05で強めのモザイク\n",
    "                    # 元のフレームに適用\n",
    "                    frame[y1:y2, x1:x2] = pixelated_face\n",
    "\n",
    "        # 結果の表示（オプション）\n",
    "        #cv2.imshow(\"Pixelated Faces\", frame)\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MPS（MacのMetal GPU）を利用可能か確認\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11の顔検出モデルをロード（verbose=False で出力なし）\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "def pixelate(image, scale=0.05):\n",
    "    \"\"\"\n",
    "    ピクセレート処理（モザイク処理）\n",
    "    :param image: 画像の一部（顔の領域）\n",
    "    :param scale: 縮小率（小さいほど強いモザイク）\n",
    "    :return: ピクセレート後の画像\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # 高さまたは幅が 0 以下の場合、処理をスキップ\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "\n",
    "    small = cv2.resize(image, (max(1, int(w * scale)), max(1, int(h * scale))), interpolation=cv2.INTER_AREA)\n",
    "    pixelated = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    return pixelated\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    動画を処理し、顔をピクセレートする\n",
    "    :param input_path: 入力動画のパス\n",
    "    :param output_path: 出力動画のパス\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # YOLOで顔検出（verbose=False）\n",
    "        results = model.predict(frame, verbose=False)\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # 無効な座標を除外\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue  # 処理しない\n",
    "\n",
    "                # 顔領域の抽出\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # ピクセレート処理\n",
    "                frame[y1:y2, x1:x2] = pixelate(face, scale=0.05)\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Pixelated Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33.9s  \n",
    "なかなか良いけど、一瞬モザイクが外れたりする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MPS（MacのMetal GPU）を利用可能か確認\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11の顔検出モデルをロード（verbose=False で出力なし）\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "def blur_face(image, ksize=(25, 25)):\n",
    "    \"\"\"\n",
    "    ガウシアンぼかし処理\n",
    "    :param image: 画像の一部（顔の領域）\n",
    "    :param ksize: カーネルサイズ（値を大きくするとぼかしが強くなる）\n",
    "    :return: ぼかし後の画像\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # 高さまたは幅が 0 の場合、処理をスキップ\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    動画を処理し、顔をガウシアンブラーでぼかす\n",
    "    :param input_path: 入力動画のパス\n",
    "    :param output_path: 出力動画のパス\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # YOLOで顔検出（verbose=False）\n",
    "        results = model.predict(frame, verbose=False)\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # 無効な座標を除外\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue  # 処理しない\n",
    "\n",
    "                # 顔領域の抽出\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "\n",
    "                # ぼかし処理\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(25, 25))\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Blurred Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.0s  \n",
    "ガウシアンブラーなのに速い・・？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MPS（MacのMetal GPU）を利用可能か確認\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11の顔検出モデルをロード（verbose=False で出力なし）\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "def blur_face(image, ksize=(25, 25)):\n",
    "    \"\"\"\n",
    "    ガウシアンぼかし処理\n",
    "    :param image: 画像の一部（顔の領域）\n",
    "    :param ksize: カーネルサイズ（値を大きくするとぼかしが強くなる）\n",
    "    :return: ぼかし後の画像\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # 高さまたは幅が 0 の場合、処理をスキップ\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    動画を処理し、5フレームごとに顔を検出し、ぼかしを適用する\n",
    "    :param input_path: 入力動画のパス\n",
    "    :param output_path: 出力動画のパス\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0  # フレームカウント\n",
    "    last_faces = []  # 検出した顔の座標を保存\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # 5フレームごとに顔検出\n",
    "        if frame_count % 5 == 0:\n",
    "            results = model.predict(frame, verbose=False)\n",
    "            last_faces = []  # 古い座標をクリア\n",
    "            for result in results:\n",
    "                for box in result.boxes.xyxy:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                    # 無効な座標を除外\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue  # 処理しない\n",
    "\n",
    "                    last_faces.append((x1, y1, x2, y2))  # 新しい座標を保存\n",
    "\n",
    "        # 最後に検出した顔座標に基づいてぼかし処理を適用\n",
    "        for (x1, y1, x2, y2) in last_faces:\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "            frame[y1:y2, x1:x2] = blur_face(face, ksize=(25, 25))\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Blurred Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.4s  \n",
    "ただし、5フレームごとに検出し、同じ箇所をぼかし続ける方法だと、ぼかりが抜ける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MPS（MacのMetal GPU）を利用可能か確認\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11の顔検出モデルをロード（verbose=False で出力なし）\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "def blur_face(image, ksize=(25, 25)):\n",
    "    \"\"\"\n",
    "    ガウシアンぼかし処理\n",
    "    :param image: 画像の一部（顔の領域）\n",
    "    :param ksize: カーネルサイズ（値を大きくするとぼかしが強くなる）\n",
    "    :return: ぼかし後の画像\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # 高さまたは幅が 0 の場合、処理をスキップ\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する\n",
    "    :param input_path: 入力動画のパス\n",
    "    :param output_path: 出力動画のパス\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0  # フレームカウント\n",
    "    face_memory = deque(maxlen=5)  # 過去5フレーム分の顔座標を保存\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []  # 現在フレームの顔座標\n",
    "\n",
    "        # 毎フレーム顔検出を行う\n",
    "        results = model.predict(frame, verbose=False)\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # 無効な座標を除外\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue  # 処理しない\n",
    "\n",
    "                current_faces.append((x1, y1, x2, y2))  # 現在検出された顔座標を保存\n",
    "\n",
    "        # 検出された顔座標を記録（過去5フレームまで保持）\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # 過去5フレーム分の顔座標をぼかす\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(25, 25))\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Blurred Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.8s  \n",
    "なかなかいいモザイクになった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MPS（MacのMetal GPU）を利用可能か確認\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11の顔検出モデルをロード（verbose=False で出力なし）\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\"\n",
    "    ガウシアンぼかし処理（範囲を広げたバージョン）\n",
    "    :param image: 画像の一部（顔の領域）\n",
    "    :param ksize: カーネルサイズ（値を大きくするとぼかしが強くなる）\n",
    "    :return: ぼかし後の画像\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return image  # 無効なサイズならそのまま返す\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\"\n",
    "    検出した顔の座標を拡張し、範囲を広げる\n",
    "    :param x1, y1, x2, y2: 元のバウンディングボックス座標\n",
    "    :param width, height: 画像サイズ\n",
    "    :param margin: 顔領域を拡張する比率（0.2 なら20%拡大）\n",
    "    :return: 拡張後の (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する\n",
    "    :param input_path: 入力動画のパス\n",
    "    :param output_path: 出力動画のパス\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0  # フレームカウント\n",
    "    face_memory = deque(maxlen=5)  # 過去5フレーム分の顔座標を保存\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []  # 現在フレームの顔座標\n",
    "\n",
    "        # 毎フレーム顔検出を行う\n",
    "        results = model.predict(frame, verbose=False)\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # 無効な座標を除外\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue  # 処理しない\n",
    "\n",
    "                # 顔の領域を拡張\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.2)\n",
    "\n",
    "                current_faces.append((x1, y1, x2, y2))  # 現在検出された顔座標を保存\n",
    "\n",
    "        # 検出された顔座標を記録（過去5フレームまで保持）\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # 過去5フレーム分の顔座標をぼかす\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Blurred Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34.5s  \n",
    "ぼかしサイズを大きくして良くなったが、遅い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11の顔検出モデル（軽量Small版）をロード（verbose=False で出力なし）\n",
    "model = YOLO(\"yolov11s-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def resize_to_yolo_size(image, stride=32):\n",
    "    \"\"\" YOLOの入力サイズに適した解像度にリサイズする \"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    new_h = int(np.ceil(h / stride) * stride)\n",
    "    new_w = int(np.ceil(w / stride) * stride)\n",
    "    return cv2.resize(image, (new_w, new_h)), (h, w)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=5)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # YOLOの入力に適したサイズにリサイズ\n",
    "        resized_frame, original_size = resize_to_yolo_size(frame)\n",
    "        original_h, original_w = original_size\n",
    "\n",
    "        # 画像をMPS（GPUメモリ）に転送\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "\n",
    "        # 顔検出（最適化設定：信頼度0.5以上, IOU閾値0.5, Half精度）\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.5, iou=0.5, half=True)\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # 検出座標を元のサイズにスケーリング\n",
    "                x1 = int(x1 * width / resized_frame.shape[1])\n",
    "                y1 = int(y1 * height / resized_frame.shape[0])\n",
    "                x2 = int(x2 * width / resized_frame.shape[1])\n",
    "                y2 = int(y2 * height / resized_frame.shape[0])\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.2)\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # ぼかし処理を並列実行\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Blurred Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1m59sもかかるようになってしまった。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano版（軽量版）を使用**\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=5)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLOの推論を最適化（画像を640×640にリサイズして高速化）**\n",
    "        resized_frame = cv2.resize(frame, (640, 640))\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "\n",
    "        # **YOLOの推論実行（half=Trueを削除）**\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.5, iou=0.5)\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # **検出結果を元の解像度に変換**\n",
    "                x1 = int(x1 * width / 640)\n",
    "                y1 = int(y1 * height / 640)\n",
    "                x2 = int(x2 * width / 640)\n",
    "                y2 = int(y2 * height / 640)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.2)\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # ぼかし処理を並列実行\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Blurred Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.8s  \n",
    "だが、精度が明らかに落ちた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Small版（精度向上版）を使用**\n",
    "model = YOLO(\"yolov11s-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=5)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLOの入力を1280x1280にリサイズ（精度向上）**\n",
    "        resized_frame = cv2.resize(frame, (1280, 1280))\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "\n",
    "        # **YOLOの推論実行（信頼度を0.4にして小さい顔も検出）**\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.4, iou=0.5)\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # **検出結果を元の解像度に変換**\n",
    "                x1 = int(x1 * width / 1280)\n",
    "                y1 = int(y1 * height / 1280)\n",
    "                x2 = int(x2 * width / 1280)\n",
    "                y2 = int(y2 * height / 1280)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.2)\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # ぼかし処理を並列実行\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Blurred Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2m3sかかるのでやはり遅い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Small版（精度向上版）を使用**\n",
    "model = YOLO(\"yolov11s-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # バッファサイズを小さくして遅延を軽減\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=5)\n",
    "\n",
    "    start_time = time.time()  # 全体の開始時間\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()  # 各フレームの開始時間\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO推論の計測開始**\n",
    "        yolo_start_time = time.time()\n",
    "\n",
    "        # **YOLOの入力を1280x1280にリサイズ（精度向上）**\n",
    "        resized_frame = cv2.resize(frame, (1280, 1280))\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "\n",
    "        # **YOLOの推論実行**\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.4, iou=0.5)\n",
    "\n",
    "        yolo_end_time = time.time()  # YOLOの処理終了時間\n",
    "\n",
    "        # **顔検出の処理時間**\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # **検出結果を元の解像度に変換**\n",
    "                x1 = int(x1 * width / 1280)\n",
    "                y1 = int(y1 * height / 1280)\n",
    "                x2 = int(x2 * width / 1280)\n",
    "                y2 = int(y2 * height / 1280)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.2)\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理の計測開始**\n",
    "        blur_start_time = time.time()\n",
    "\n",
    "        # ぼかし処理を並列実行\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "\n",
    "        blur_end_time = time.time()  # ぼかし処理の終了時間\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        # 結果の表示（オプション） ※コメントアウト\n",
    "        # cv2.imshow(\"Blurred Faces\", frame)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()  # フレームの処理終了時間\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **10フレームごとにログを出力**\n",
    "        if frame_count % 10 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1m52sかかる。ログ出力版。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano版（高速版）を使用**\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=5)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO推論の計測開始**\n",
    "        yolo_start_time = time.time()\n",
    "\n",
    "        # **YOLOの入力を 960x960 にリサイズ（高速化）**\n",
    "        resized_frame = cv2.resize(frame, (960, 960))\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "\n",
    "        # **YOLOの推論実行**\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.4, iou=0.5)\n",
    "\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # **検出結果を元の解像度に変換**\n",
    "                x1 = int(x1 * width / 960)\n",
    "                y1 = int(y1 * height / 960)\n",
    "                x2 = int(x2 * width / 960)\n",
    "                y2 = int(y2 * height / 960)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.2)\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理の計測開始**\n",
    "        blur_start_time = time.time()\n",
    "\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "51sで済んだ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Small版（精度向上版）を使用**\n",
    "model = YOLO(\"yolov11s-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=5)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO推論の計測開始**\n",
    "        yolo_start_time = time.time()\n",
    "\n",
    "        # **YOLOの入力を 1280x1280 にリサイズ（精度向上）**\n",
    "        resized_frame = cv2.resize(frame, (1280, 1280))\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "\n",
    "        # **YOLOの推論実行（conf=0.35 に変更して小さい顔を拾いやすくする）**\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.35, iou=0.5)\n",
    "\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "\n",
    "                # **検出結果を元の解像度に変換**\n",
    "                x1 = int(x1 * width / 1280)\n",
    "                y1 = int(y1 * height / 1280)\n",
    "                x2 = int(x2 * width / 1280)\n",
    "                y2 = int(y2 * height / 1280)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.2)\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理の計測開始**\n",
    "        blur_start_time = time.time()\n",
    "\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2m3s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Small版（精度向上版）を使用**\n",
    "model = YOLO(\"yolov11s-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    if h == 0 or w == 0:\n",
    "        return image\n",
    "    return cv2.GaussianBlur(image, ksize, 30)\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=5)\n",
    "    batch_frames = []\n",
    "    original_sizes = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **リサイズの計測開始**\n",
    "        resize_start_time = time.time()\n",
    "\n",
    "        # **YOLOの入力を 1280x1280 にリサイズ（最適化）**\n",
    "        resized_frame = cv2.resize(frame, (1280, 1280), interpolation=cv2.INTER_AREA)\n",
    "        batch_frames.append(resized_frame)\n",
    "        original_sizes.append((frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **バッチ処理（複数フレームをまとめて処理）**\n",
    "        if len(batch_frames) >= batch_size:\n",
    "            batch_tensor = torch.stack([torch.from_numpy(f).permute(2, 0, 1).to(device).float() / 255.0 for f in batch_frames])\n",
    "            \n",
    "            # **YOLOの推論実行**\n",
    "            yolo_start_time = time.time()\n",
    "            results = model.predict(batch_tensor, verbose=False, conf=0.35, iou=0.5)\n",
    "            yolo_end_time = time.time()\n",
    "            yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "            # **各フレームの顔座標を取得**\n",
    "            for i, result in enumerate(results):\n",
    "                orig_w, orig_h = original_sizes[i]\n",
    "                faces = []\n",
    "\n",
    "                for box in result.boxes.xyxy:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    x1 = int(x1 * orig_w / 1280)\n",
    "                    y1 = int(y1 * orig_h / 1280)\n",
    "                    x2 = int(x2 * orig_w / 1280)\n",
    "                    y2 = int(y2 * orig_h / 1280)\n",
    "\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > orig_w or y2 > orig_h:\n",
    "                        continue\n",
    "                    x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, orig_w, orig_h, margin=0.2)\n",
    "                    faces.append((x1, y1, x2, y2))\n",
    "\n",
    "                face_memory.append(faces)\n",
    "\n",
    "            batch_frames = []\n",
    "            original_sizes = []\n",
    "\n",
    "        # **ぼかし処理の計測開始**\n",
    "        blur_start_time = time.time()\n",
    "\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\", batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1m15s  \n",
    "バッチ処理は微妙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import concurrent.futures\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano版（高速版）を使用**\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 30) if image.size > 0 else image\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.2):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def resize_frame(frame):\n",
    "    \"\"\" マルチスレッドでリサイズ処理 \"\"\"\n",
    "    return cv2.resize(frame, (960, 960), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出し、最大5フレームまでぼかしを維持する \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=10)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        while cap.isOpened():\n",
    "            frame_start_time = time.time()\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            current_faces = []\n",
    "\n",
    "            # **リサイズの計測開始**\n",
    "            resize_start_time = time.time()\n",
    "            future_resized_frame = executor.submit(resize_frame, frame)  # 並列リサイズ\n",
    "            resized_frame = future_resized_frame.result()  # 結果を取得\n",
    "            resize_end_time = time.time()\n",
    "            resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "            # **YOLOの推論実行（最適化設定を適用）**\n",
    "            yolo_start_time = time.time()\n",
    "            frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device, non_blocking=True).float() / 255.0\n",
    "            results = model.predict(frame_tensor, verbose=False, conf=0.3, iou=0.4, max_det=5, agnostic_nms=True)\n",
    "            yolo_end_time = time.time()\n",
    "            yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "            for result in results:\n",
    "                for box in result.boxes.xyxy:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    x1 = int(x1 * width / 960)\n",
    "                    y1 = int(y1 * height / 960)\n",
    "                    x2 = int(x2 * width / 960)\n",
    "                    y2 = int(y2 * height / 960)\n",
    "\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.2)\n",
    "                    current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "            face_memory.append(current_faces)\n",
    "\n",
    "            # **ぼかし処理の計測開始（並列処理）**\n",
    "            blur_start_time = time.time()\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as blur_executor:\n",
    "                future_blur_faces = {\n",
    "                    blur_executor.submit(blur_face, frame[y1:y2, x1:x2]): (x1, y1, x2, y2)\n",
    "                    for faces in face_memory for (x1, y1, x2, y2) in faces\n",
    "                }\n",
    "                for future in concurrent.futures.as_completed(future_blur_faces):\n",
    "                    x1, y1, x2, y2 = future_blur_faces[future]\n",
    "                    frame[y1:y2, x1:x2] = future.result()\n",
    "\n",
    "            blur_end_time = time.time()\n",
    "            blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "            frame_end_time = time.time()\n",
    "            total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "            # **100フレームごとにログを出力**\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "                print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2分動画を11m30s  \n",
    "5フレームモザイクから10フレームモザイクへ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、モザイクを10フレームまで残すようにし、\n",
    "モザイクの範囲をさらに大きくして顔検出を5フレームに一回にしたらどうなる？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import concurrent.futures\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano版（高速版）を使用**\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(45, 45)):\n",
    "    \"\"\" ガウシアンぼかし処理（ぼかし強度アップ） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 30) if image.size > 0 else image\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.3):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる（範囲拡大）\"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def resize_frame(frame):\n",
    "    \"\"\" マルチスレッドでリサイズ処理 \"\"\"\n",
    "    return cv2.resize(frame, (960, 960), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、5フレームに1回顔を検出、10フレームまでモザイクを維持 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=10)  # **モザイクを10フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        while cap.isOpened():\n",
    "            frame_start_time = time.time()\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            current_faces = []\n",
    "\n",
    "            # **リサイズの計測開始**\n",
    "            resize_start_time = time.time()\n",
    "            future_resized_frame = executor.submit(resize_frame, frame)  # 並列リサイズ\n",
    "            resized_frame = future_resized_frame.result()  # 結果を取得\n",
    "            resize_end_time = time.time()\n",
    "            resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "            # **5フレームに1回顔検出**\n",
    "            if frame_count % 5 == 0:\n",
    "                yolo_start_time = time.time()\n",
    "                frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device, non_blocking=True).float() / 255.0\n",
    "                results = model.predict(frame_tensor, verbose=False, conf=0.3, iou=0.4, max_det=5, agnostic_nms=True)\n",
    "                yolo_end_time = time.time()\n",
    "                yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "                for result in results:\n",
    "                    for box in result.boxes.xyxy:\n",
    "                        x1, y1, x2, y2 = map(int, box)\n",
    "                        x1 = int(x1 * width / 960)\n",
    "                        y1 = int(y1 * height / 960)\n",
    "                        x2 = int(x2 * width / 960)\n",
    "                        y2 = int(y2 * height / 960)\n",
    "\n",
    "                        if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                            continue\n",
    "                        x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.3)\n",
    "                        current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "                face_memory.append(current_faces)\n",
    "\n",
    "            # **ぼかし処理の計測開始（並列処理）**\n",
    "            blur_start_time = time.time()\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as blur_executor:\n",
    "                future_blur_faces = {\n",
    "                    blur_executor.submit(blur_face, frame[y1:y2, x1:x2]): (x1, y1, x2, y2)\n",
    "                    for faces in face_memory for (x1, y1, x2, y2) in faces\n",
    "                }\n",
    "                for future in concurrent.futures.as_completed(future_blur_faces):\n",
    "                    x1, y1, x2, y2 = future_blur_faces[future]\n",
    "                    frame[y1:y2, x1:x2] = future.result()\n",
    "\n",
    "            blur_end_time = time.time()\n",
    "            blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "            frame_end_time = time.time()\n",
    "            total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "            # **100フレームごとにログを出力**\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "                print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2分の動画を5m18s  \n",
    "悪くないが、顔が隠れていない箇所がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLOv8n-face、YOLOの設定変更（ただし、max_det=3はやめる）、YOLOの処理を並列化、ThreadPoolExecutor利用のプログラムを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import concurrent.futures\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化（メモリ管理を考慮）\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv8n-face（高速版）を使用**\n",
    "model = YOLO(\"yolov8n-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(45, 45)):\n",
    "    \"\"\" ガウシアンぼかし処理（ぼかし強度アップ） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 30) if image.size > 0 else image\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.3):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる（範囲拡大）\"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def resize_frame(frame):\n",
    "    \"\"\" マルチスレッドでリサイズ処理 \"\"\"\n",
    "    return cv2.resize(frame, (960, 960), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def yolo_inference(frame):\n",
    "    \"\"\" YOLOの推論（メモリ管理を考慮）\"\"\"\n",
    "    torch.mps.empty_cache()  # **不要なメモリを解放**\n",
    "    frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "    results = model.predict(frame_tensor, verbose=False, conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "    return results\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、MPSのメモリ負荷を軽減 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=10)  # **モザイクを10フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as yolo_executor, concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        while cap.isOpened():\n",
    "            frame_start_time = time.time()\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            current_faces = []\n",
    "\n",
    "            # **リサイズの計測開始（並列実行）**\n",
    "            resize_start_time = time.time()\n",
    "            future_resized_frame = executor.submit(resize_frame, frame)\n",
    "            resized_frame = future_resized_frame.result()\n",
    "            resize_end_time = time.time()\n",
    "            resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "            # **YOLOの推論を並列実行（ただし1スレッドのみ）**\n",
    "            yolo_start_time = time.time()\n",
    "            future_yolo = yolo_executor.submit(yolo_inference, resized_frame)\n",
    "            results = future_yolo.result()\n",
    "            yolo_end_time = time.time()\n",
    "            yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "            for result in results:\n",
    "                for box in result.boxes.xyxy:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    x1 = int(x1 * width / 960)\n",
    "                    y1 = int(y1 * height / 960)\n",
    "                    x2 = int(x2 * width / 960)\n",
    "                    y2 = int(y2 * height / 960)\n",
    "\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.3)\n",
    "                    current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "            face_memory.append(current_faces)\n",
    "\n",
    "            # **ぼかし処理の計測開始（並列処理）**\n",
    "            blur_start_time = time.time()\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as blur_executor:\n",
    "                future_blur_faces = {\n",
    "                    blur_executor.submit(blur_face, frame[y1:y2, x1:x2]): (x1, y1, x2, y2)\n",
    "                    for faces in face_memory for (x1, y1, x2, y2) in faces\n",
    "                }\n",
    "                for future in concurrent.futures.as_completed(future_blur_faces):\n",
    "                    x1, y1, x2, y2 = future_blur_faces[future]\n",
    "                    frame[y1:y2, x1:x2] = future.result()\n",
    "\n",
    "            blur_end_time = time.time()\n",
    "            blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "            frame_end_time = time.time()\n",
    "            total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "            # **100フレームごとにログを出力**\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "                print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2分動画で14m42s  \n",
    "性能はやはりYOLOv11の方がいいように思える\n",
    "\n",
    "試しに同じ条件でYOLOv11では？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import concurrent.futures\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3のGPU（Metal）を有効化（メモリ管理を考慮）\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv8n-face（高速版）を使用**\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCVの並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(45, 45)):\n",
    "    \"\"\" ガウシアンぼかし処理（ぼかし強度アップ） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 30) if image.size > 0 else image\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.3):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる（範囲拡大）\"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def resize_frame(frame):\n",
    "    \"\"\" マルチスレッドでリサイズ処理 \"\"\"\n",
    "    return cv2.resize(frame, (960, 960), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def yolo_inference(frame):\n",
    "    \"\"\" YOLOの推論（メモリ管理を考慮）\"\"\"\n",
    "    torch.mps.empty_cache()  # **不要なメモリを解放**\n",
    "    frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "    results = model.predict(frame_tensor, verbose=False, conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "    return results\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、MPSのメモリ負荷を軽減 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=10)  # **モザイクを10フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as yolo_executor, concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        while cap.isOpened():\n",
    "            frame_start_time = time.time()\n",
    "\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            current_faces = []\n",
    "\n",
    "            # **リサイズの計測開始（並列実行）**\n",
    "            resize_start_time = time.time()\n",
    "            future_resized_frame = executor.submit(resize_frame, frame)\n",
    "            resized_frame = future_resized_frame.result()\n",
    "            resize_end_time = time.time()\n",
    "            resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "            # **YOLOの推論を並列実行（ただし1スレッドのみ）**\n",
    "            yolo_start_time = time.time()\n",
    "            future_yolo = yolo_executor.submit(yolo_inference, resized_frame)\n",
    "            results = future_yolo.result()\n",
    "            yolo_end_time = time.time()\n",
    "            yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "            for result in results:\n",
    "                for box in result.boxes.xyxy:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    x1 = int(x1 * width / 960)\n",
    "                    y1 = int(y1 * height / 960)\n",
    "                    x2 = int(x2 * width / 960)\n",
    "                    y2 = int(y2 * height / 960)\n",
    "\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.3)\n",
    "                    current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "            face_memory.append(current_faces)\n",
    "\n",
    "            # **ぼかし処理の計測開始（並列処理）**\n",
    "            blur_start_time = time.time()\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as blur_executor:\n",
    "                future_blur_faces = {\n",
    "                    blur_executor.submit(blur_face, frame[y1:y2, x1:x2]): (x1, y1, x2, y2)\n",
    "                    for faces in face_memory for (x1, y1, x2, y2) in faces\n",
    "                }\n",
    "                for future in concurrent.futures.as_completed(future_blur_faces):\n",
    "                    x1, y1, x2, y2 = future_blur_faces[future]\n",
    "                    frame[y1:y2, x1:x2] = future.result()\n",
    "\n",
    "            blur_end_time = time.time()\n",
    "            blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "            frame_end_time = time.time()\n",
    "            total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "            # **100フレームごとにログを出力**\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "                print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2分の動画で18m54s  \n",
    "遅い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### あたらめてモデルを一旦yolo11n.ptにする。\n",
    "- ✅ マルチスレッドを完全に排除し、シンプルなシングルスレッド処理に\n",
    "- ✅ モデルを yolo11n.pt に変更し、yolov8n-face.pt から変更\n",
    "- ✅ torch.mps.empty_cache() を適用し、Metal MPS のメモリ問題を解決\n",
    "- ✅ リサイズを 960x960 に統一し、YOLO の入力サイズを最適化\n",
    "- ✅ YOLO の設定を最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化（ただしシングルスレッド処理）\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(45, 45)):\n",
    "    \"\"\" ガウシアンぼかし処理（ぼかし強度アップ） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 30) if image.size > 0 else image\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.3):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる（範囲拡大）\"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、最適化されたシングルスレッド処理で実行 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=10)  # **モザイクを10フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **リサイズ**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (960, 960), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.3, iou=0.4, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 960)\n",
    "                y1 = int(y1 * height / 960)\n",
    "                x2 = int(x2 * width / 960)\n",
    "                y2 = int(y2 * height / 960)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.3)\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(45, 45))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結果\n",
    "```\n",
    "Frame 500/615 - Resize: 0.002s, YOLO: 0.049s, Blur: 0.251s, Total: 0.334s\n",
    "Estimated remaining time: 38.3 seconds\n",
    "\n",
    "Total processing time: 201.9 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修正点（Total processing time: 69.4 seconds）\n",
    "- ✅ モザイクの保持時間を 10 フレーム → 2 フレーム に短縮\n",
    "- ✅ ぼかし処理のカーネルサイズを (45,45) → (35,35) に変更し、計算負荷を軽減\n",
    "- ✅ YOLO の設定は変更せず、処理の最適化に集中\n",
    "- ✅ シンプルなシングルスレッド処理を維持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理（カーネルサイズを適正化） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 30) if image.size > 0 else image\n",
    "\n",
    "def expand_bbox(x1, y1, x2, y2, width, height, margin=0.3):\n",
    "    \"\"\" 検出した顔の座標を拡張し、範囲を広げる \"\"\"\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    x1 = max(0, int(x1 - w * margin))\n",
    "    y1 = max(0, int(y1 - h * margin))\n",
    "    x2 = min(width, int(x2 + w * margin))\n",
    "    y2 = min(height, int(y2 + h * margin))\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、モザイクを2フレーム保持 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=2)  # **モザイクを2フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **リサイズ**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (960, 960), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.3, iou=0.4, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 960)\n",
    "                y1 = int(y1 * height / 960)\n",
    "                x2 = int(x2 * width / 960)\n",
    "                y2 = int(y2 * height / 960)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                x1, y1, x2, y2 = expand_bbox(x1, y1, x2, y2, width, height, margin=0.3)\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（モザイクを2フレームまで保持）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))  # **カーネルサイズを変更**\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結果\n",
    "```\n",
    "Frame 500/615 - Resize: 0.002s, YOLO: 0.044s, Blur: 0.033s, Total: 0.106s\n",
    "Estimated remaining time: 13.2 seconds\n",
    "\n",
    "Total processing time: 69.4 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修正点（Total processing time: 64.1 seconds）\n",
    "- ✅ ぼかし範囲の拡大をやめ、元のサイズに戻した\n",
    "- ✅ その他の最適化（YOLO のパラメータ、リサイズ、MPS メモリ管理）はそのまま維持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化（シングルスレッド処理）\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(35, 35)):\n",
    "    \"\"\" ガウシアンぼかし処理 \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 30) if image.size > 0 else image\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、モザイクを2フレーム保持 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=2)  # **モザイクを2フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **リサイズ**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (960, 960), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.3, iou=0.4, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 960)\n",
    "                y1 = int(y1 * height / 960)\n",
    "                x2 = int(x2 * width / 960)\n",
    "                y2 = int(y2 * height / 960)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（範囲を拡大せず、元のサイズのまま）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(35, 35))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修正（Total processing time: 47.5 seconds）\n",
    "- YOLO の入力解像度を下げる\n",
    "- YOLO の設定をさらに最適化\n",
    "- ぼかし処理の最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(25, 25)):\n",
    "    \"\"\" ガウシアンぼかし処理（高速化） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 15) if image.size > 0 else image\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速化を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=2)  # **モザイクを2フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO の入力サイズを 640x640 に変更**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 640)\n",
    "                y1 = int(y1 * height / 640)\n",
    "                x2 = int(x2 * width / 640)\n",
    "                y2 = int(y2 * height / 640)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（カーネルサイズを小さく）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(25, 25))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 修正（Total processing time: 65.4 seconds）\n",
    "- ✅ モザイク（ピクセレート）を適用しつつ、バイラテラルフィルタで滑らかに\n",
    "- ✅ 処理負荷を最小限に抑える（高速化）\n",
    "- ✅ 背景とモザイクが自然になじむように最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化（シングルスレッド処理）\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def pixelate_face(image, pixel_size=10):\n",
    "    \"\"\" ピクセレート（モザイク）処理 \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    small = cv2.resize(image, (w // pixel_size, h // pixel_size), interpolation=cv2.INTER_LINEAR)\n",
    "    return cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "def pixelate_with_smooth(image, pixel_size=10):\n",
    "    \"\"\" ピクセレート＋バイラテラルフィルタで自然になじませる \"\"\"\n",
    "    pixelated = pixelate_face(image, pixel_size)\n",
    "    return cv2.bilateralFilter(pixelated, 9, 75, 75)  # なじませる\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速モザイク処理を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=2)  # **モザイクを2フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO の入力サイズを 640x640 に変更**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 640)\n",
    "                y1 = int(y1 * height / 640)\n",
    "                x2 = int(x2 * width / 640)\n",
    "                y2 = int(y2 * height / 640)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **モザイク処理（Pixelation + Bilateral Filter）**\n",
    "        mosaic_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = pixelate_with_smooth(face, pixel_size=10)  # **モザイク＋バイラテラル**\n",
    "        mosaic_end_time = time.time()\n",
    "        mosaic_processing_time = mosaic_end_time - mosaic_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Mosaic: {mosaic_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メモ\n",
    "- モザイクはガウスを採用\n",
    "- モザイクは最大5フレーム保持したい\n",
    "- 2フレームごとに検出\n",
    "- 2フレーム目は同じ箇所にモザイク\n",
    "- （範囲を少しだけ拡大？）\n",
    "- マルチスレッドを完全にやめれていない？\n",
    "- もはや顔検出ではなく、人物全体にする\n",
    "- python3.13ではどうだ？\n",
    "- yolov11s.ptではどうだ？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
