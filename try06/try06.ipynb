{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改善案（Total processing time: 54.7 seconds）\n",
    "\n",
    "|改善ポイント|方法|効果|\n",
    "|---|---|---|\n",
    "|YOLOの前処理を最適化|torch.from_numpy() 削減|CPU負荷軽減（メモリオーバーヘッド削減）|\n",
    "|YOLOのバッチ処理|5フレームごとに推論|YOLO推論回数を1/5に削減（高速化）|\n",
    "|並列処理|ThreadPoolExecutor を使用|CPUとGPUの同時活用（処理効率UP）|\n",
    "|動画のエンコード最適化|H.264 コーデックを使用|動画ファイルの圧縮効率UP・書き出し高速化|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(25, 25)):\n",
    "    \"\"\" ガウシアンぼかし処理（高速化） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 15) if image.size > 0 else image\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速化を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=4)  # **モザイクを4フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO の入力サイズを 640x640 に変更**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 640)\n",
    "                y1 = int(y1 * height / 640)\n",
    "                x2 = int(x2 * width / 640)\n",
    "                y2 = int(y2 * height / 640)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（カーネルサイズを小さく）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(25, 25))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - Resize: 0.003s, YOLO: 0.023s, Blur: 0.021s, Total: 0.069s\n",
    "Estimated remaining time: 46.6 seconds\n",
    "Frame 200/615 - Resize: 0.002s, YOLO: 0.022s, Blur: 0.021s, Total: 0.070s\n",
    "Estimated remaining time: 37.0 seconds\n",
    "Frame 300/615 - Resize: 0.003s, YOLO: 0.036s, Blur: 0.023s, Total: 0.087s\n",
    "Estimated remaining time: 28.6 seconds\n",
    "Frame 400/615 - Resize: 0.002s, YOLO: 0.019s, Blur: 0.022s, Total: 0.070s\n",
    "Estimated remaining time: 19.6 seconds\n",
    "Frame 500/615 - Resize: 0.002s, YOLO: 0.039s, Blur: 0.025s, Total: 0.097s\n",
    "Estimated remaining time: 10.5 seconds\n",
    "\n",
    "Total processing time: 54.7 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改善のポイント（Total processing time: 58.0 seconds）\n",
    "- ✅ YOLOの内部処理を活かし、推論高速化\n",
    "- ✅ MPSのメモリ管理を適切化\n",
    "- ✅ フレームの読み込みを非同期化し、ボトルネック解消\n",
    "- ✅ GaussianBlurを並列化して処理時間短縮\n",
    "\n",
    "結論：微妙だった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(25, 25)):\n",
    "    \"\"\" ガウシアンぼかし処理（高速化） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 15) if image.size > 0 else image\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速化を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=4)  # **モザイクを4フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO の入力サイズを 640x640 に変更**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 640)\n",
    "                y1 = int(y1 * height / 640)\n",
    "                x2 = int(x2 * width / 640)\n",
    "                y2 = int(y2 * height / 640)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（カーネルサイズを小さく）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(25, 25))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - Resize: 0.002s, YOLO: 0.023s, Blur: 0.020s, Total: 0.068s\n",
    "Estimated remaining time: 54.2 seconds\n",
    "Frame 200/615 - Resize: 0.002s, YOLO: 0.025s, Blur: 0.021s, Total: 0.073s\n",
    "Estimated remaining time: 41.7 seconds\n",
    "Frame 300/615 - Resize: 0.002s, YOLO: 0.029s, Blur: 0.023s, Total: 0.079s\n",
    "Estimated remaining time: 31.5 seconds\n",
    "Frame 400/615 - Resize: 0.002s, YOLO: 0.032s, Blur: 0.021s, Total: 0.082s\n",
    "Estimated remaining time: 21.2 seconds\n",
    "Frame 500/615 - Resize: 0.002s, YOLO: 0.037s, Blur: 0.022s, Total: 0.088s\n",
    "Estimated remaining time: 11.2 seconds\n",
    "\n",
    "Total processing time: 58.0 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結局、顔検出のみにする（Total processing time: 84.3 seconds）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCV のスレッド数を最適化\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"YOLOのstride=32の倍数になるようリサイズ\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width = (width // 32) * 32 + (32 if width % 32 != 0 else 0)\n",
    "    return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR), new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\" 顔部分のサイズを縮小してぼかしをかけてから元サイズに戻す \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, (ksize[0], ksize[1]), interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速化を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=4)  # **モザイクを4フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLOの推論（stride 32 の倍数にリサイズ）**\n",
    "        yolo_start_time = time.time()\n",
    "        \n",
    "        if frame_count % 50 == 0:\n",
    "            torch.mps.empty_cache()  # **50フレームごとにキャッシュクリア**\n",
    "\n",
    "        resized_frame, new_width, new_height = resize_to_stride32(frame)\n",
    "\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, imgsz=(new_height, new_width), conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        \n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                # リサイズ前の座標に戻す\n",
    "                x1 = int(x1 * width / new_width)\n",
    "                y1 = int(y1 * height / new_height)\n",
    "                x2 = int(x2 * width / new_width)\n",
    "                y2 = int(y2 * height / new_height)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（顔領域を縮小→ぼかし→拡大）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(15, 15))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - YOLO: 0.046s, Blur: 0.000s, Total: 0.064s\n",
    "Estimated remaining time: 51.5 seconds\n",
    "Frame 200/615 - YOLO: 0.049s, Blur: 0.000s, Total: 0.068s\n",
    "Estimated remaining time: 37.4 seconds\n",
    "Frame 300/615 - YOLO: 0.119s, Blur: 0.001s, Total: 0.153s\n",
    "Estimated remaining time: 33.0 seconds\n",
    "Frame 400/615 - YOLO: 0.119s, Blur: 0.001s, Total: 0.153s\n",
    "Estimated remaining time: 26.2 seconds\n",
    "Frame 500/615 - YOLO: 0.130s, Blur: 0.001s, Total: 0.163s\n",
    "Estimated remaining time: 15.3 seconds\n",
    "\n",
    "Total processing time: 84.3 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結局顔のみ検出の方が精度良い  \n",
    "改善を1個ずつ試していこう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修正（80.4 seconds）\n",
    "- データ型の最適化: resized_frame を torch.Tensor に変換する際、float32 型に変換し、  \n",
    "  値を 0-1 の範囲にスケーリングしています。これにより、モデルが期待する入力形式に合わせています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11 Nano（yolov11n-face.pt）を使用\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCV のスレッド数を最適化\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"YOLOのstride=32の倍数になるようリサイズ\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width = (width // 32) * 32 + (32 if width % 32 != 0 else 0)\n",
    "    return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR), new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\" 顔部分のサイズを縮小してぼかしをかけてから元サイズに戻す \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, (ksize[0], ksize[1]), interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速化を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=4)  # モザイクを4フレームまで保持\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # YOLOの推論（stride 32 の倍数にリサイズ）\n",
    "        yolo_start_time = time.time()\n",
    "        \n",
    "        if frame_count % 50 == 0:\n",
    "            torch.mps.empty_cache()  # 50フレームごとにキャッシュクリア\n",
    "\n",
    "        resized_frame, new_width, new_height = resize_to_stride32(frame)\n",
    "\n",
    "        # データ型の最適化\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, imgsz=(new_height, new_width), conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        \n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                # リサイズ前の座標に戻す\n",
    "                x1 = int(x1 * width / new_width)\n",
    "                y1 = int(y1 * height / new_height)\n",
    "                x2 = int(x2 * width / new_width)\n",
    "                y2 = int(y2 * height / new_height)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # ぼかし処理（顔領域を縮小→ぼかし→拡大）\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(15, 15))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # 100フレームごとにログを出力\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - YOLO: 0.046s, Blur: 0.000s, Total: 0.064s\n",
    "Estimated remaining time: 46.0 seconds\n",
    "Frame 200/615 - YOLO: 0.049s, Blur: 0.000s, Total: 0.067s\n",
    "Estimated remaining time: 34.8 seconds\n",
    "Frame 300/615 - YOLO: 0.118s, Blur: 0.001s, Total: 0.151s\n",
    "Estimated remaining time: 30.1 seconds\n",
    "Frame 400/615 - YOLO: 0.117s, Blur: 0.001s, Total: 0.148s\n",
    "Estimated remaining time: 24.6 seconds\n",
    "Frame 500/615 - YOLO: 0.124s, Blur: 0.001s, Total: 0.160s\n",
    "Estimated remaining time: 14.5 seconds\n",
    "\n",
    "Total processing time: 80.4 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOモデルのバッチ処理を導入（45.9 seconds）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11 Nano（yolov11n-face.pt）を使用\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCV のスレッド数を最適化\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"YOLOのstride=32の倍数になるようリサイズ\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width = (width // 32) * 32 + (32 if width % 32 != 0 else 0)\n",
    "    return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR), new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"顔部分のサイズを縮小してぼかしをかけてから元サイズに戻す\"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    \"\"\"動画を処理し、毎フレーム顔を検出しながら、高速化を実施\"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=4)  # モザイクを4フレームまで保持\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    frame_batch = []\n",
    "    original_frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # フレームをリサイズしてバッチに追加\n",
    "        resized_frame, new_width, new_height = resize_to_stride32(frame)\n",
    "        frame_batch.append(resized_frame)\n",
    "        original_frames.append(frame)\n",
    "\n",
    "        # バッチサイズに達するか、最後のフレームの場合に推論を実行\n",
    "        if len(frame_batch) == batch_size or frame_count == total_frames:\n",
    "            # YOLOの推論\n",
    "            yolo_start_time = time.time()\n",
    "\n",
    "            # バッチ内のフレームをテンソルに変換\n",
    "            frame_tensors = [torch.from_numpy(f).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0 for f in frame_batch]\n",
    "            batch_tensor = torch.cat(frame_tensors, dim=0)\n",
    "\n",
    "            # 推論を実行\n",
    "            results = model.predict(batch_tensor, verbose=False, imgsz=(new_width, new_height), conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "\n",
    "            yolo_end_time = time.time()\n",
    "            yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "            # 各フレームの結果を処理\n",
    "            for i, result in enumerate(results):\n",
    "                faces = []\n",
    "                for box in result.boxes.xyxy:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    # リサイズ前の座標に戻す\n",
    "                    x1 = int(x1 * width / new_width)\n",
    "                    y1 = int(y1 * height / new_height)\n",
    "                    x2 = int(x2 * width / new_width)\n",
    "                    y2 = int(y2 * height / new_height)\n",
    "\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    faces.append((x1, y1, x2, y2))\n",
    "\n",
    "                face_memory.append(faces)\n",
    "\n",
    "                # ぼかし処理（顔領域を縮小→ぼかし→拡大）\n",
    "                blur_start_time = time.time()\n",
    "                for faces in face_memory:\n",
    "                    for (x1, y1, x2, y2) in faces:\n",
    "                        face = original_frames[i][y1:y2, x1:x2]\n",
    "                        original_frames[i][y1:y2, x1:x2] = blur_face(face, ksize=(15, 15))\n",
    "                blur_end_time = time.time()\n",
    "                blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "                out.write(original_frames[i])\n",
    "\n",
    "            # 100フレームごとにログを出力\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s\")\n",
    "                print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "            # バッチをクリア\n",
    "            frame_batch = []\n",
    "            original_frames = []\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - YOLO: 0.200s, Blur: 0.000s\n",
    "Estimated remaining time: 43.9 seconds\n",
    "Frame 200/615 - YOLO: 0.187s, Blur: 0.000s\n",
    "Estimated remaining time: 32.9 seconds\n",
    "Frame 300/615 - YOLO: 0.182s, Blur: 0.000s\n",
    "Estimated remaining time: 24.2 seconds\n",
    "Frame 400/615 - YOLO: 0.181s, Blur: 0.000s\n",
    "Estimated remaining time: 16.5 seconds\n",
    "Frame 500/615 - YOLO: 0.203s, Blur: 0.001s\n",
    "Estimated remaining time: 8.8 seconds\n",
    "\n",
    "Total processing time: 45.9 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顔の位置補完（IOU利用）（45.1 seconds）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11 Nano（yolov11n-face.pt）を使用\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCV のスレッド数を最適化\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"YOLOのstride=32の倍数になるようリサイズ\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width = (width // 32) * 32 + (32 if width % 32 != 0 else 0)\n",
    "    return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR), new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"顔部分のサイズを縮小してぼかしをかけてから元サイズに戻す\"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    2つのバウンディングボックス(boxA, boxB)に対するIoU(Intersection over Union)を計算\n",
    "    box = (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=3):\n",
    "    \"\"\"\n",
    "    前フレームまでの顔領域リスト(face_tracks)に、今フレームの検出結果(new_dets)を反映して更新する。\n",
    "    face_tracks: [(x1, y1, x2, y2, life), ... ]\n",
    "    new_dets: [(x1, y1, x2, y2), ... ]\n",
    "    iou_thresh: IoUがこの値以上なら同じ顔とみなす\n",
    "    max_life: ライフ上限(継続保持するフレーム数)\n",
    "    \"\"\"\n",
    "    updated_tracks = []\n",
    "\n",
    "    # まず既存トラックをライフを1減らして用意\n",
    "    for (tx1, ty1, tx2, ty2, life) in face_tracks:\n",
    "        updated_tracks.append([tx1, ty1, tx2, ty2, life - 1])\n",
    "\n",
    "    # new_dets と既存トラックをマッチング\n",
    "    for (nx1, ny1, nx2, ny2) in new_dets:\n",
    "        best_iou = 0\n",
    "        best_index = -1\n",
    "        for i, (tx1, ty1, tx2, ty2, life) in enumerate(updated_tracks):\n",
    "            iou_val = compute_iou((tx1, ty1, tx2, ty2), (nx1, ny1, nx2, ny2))\n",
    "            if iou_val > best_iou:\n",
    "                best_iou = iou_val\n",
    "                best_index = i\n",
    "        # IOUが一定以上あれば既存トラックを更新\n",
    "        if best_iou >= iou_thresh and best_index >= 0:\n",
    "            updated_tracks[best_index][0] = nx1\n",
    "            updated_tracks[best_index][1] = ny1\n",
    "            updated_tracks[best_index][2] = nx2\n",
    "            updated_tracks[best_index][3] = ny2\n",
    "            updated_tracks[best_index][4] = max_life  # 検出されたのでライフ復活\n",
    "        else:\n",
    "            # 新しい顔として追加\n",
    "            updated_tracks.append([nx1, ny1, nx2, ny2, max_life])\n",
    "\n",
    "    # lifeが0以下のトラックを削除\n",
    "    filtered_tracks = []\n",
    "    for t in updated_tracks:\n",
    "        if t[4] > 0:\n",
    "            filtered_tracks.append(t)\n",
    "\n",
    "    return filtered_tracks\n",
    "\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    \"\"\"動画を処理し、毎フレーム顔を検出しながら、高速化＋顔位置補完(IOU利用)を実施\"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # -------------------------------------\n",
    "    # 顔領域を補完するためのトラック管理リスト\n",
    "    # [(x1, y1, x2, y2, life), ...]\n",
    "    # life は一定フレーム数まで未検出でも保持するために使う\n",
    "    # -------------------------------------\n",
    "    face_tracks = []\n",
    "    max_life = 4   # 4フレーム程度未検出でも補完する例\n",
    "\n",
    "    frame_batch = []\n",
    "    original_frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # フレームをリサイズしてバッチに追加\n",
    "        resized_frame, new_width, new_height = resize_to_stride32(frame)\n",
    "        frame_batch.append(resized_frame)\n",
    "        original_frames.append(frame)\n",
    "\n",
    "        # バッチサイズに達するか、最後のフレームの場合に推論を実行\n",
    "        if len(frame_batch) == batch_size or frame_count == total_frames:\n",
    "            # YOLOの推論\n",
    "            yolo_start_time = time.time()\n",
    "\n",
    "            # バッチ内のフレームをテンソルに変換\n",
    "            frame_tensors = [torch.from_numpy(f).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0 for f in frame_batch]\n",
    "            batch_tensor = torch.cat(frame_tensors, dim=0)\n",
    "\n",
    "            # 推論を実行\n",
    "            results = model.predict(\n",
    "                batch_tensor, \n",
    "                verbose=False, \n",
    "                imgsz=(new_width, new_height),\n",
    "                conf=0.25, \n",
    "                iou=0.3, \n",
    "                agnostic_nms=True\n",
    "            )\n",
    "\n",
    "            yolo_end_time = time.time()\n",
    "            yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "            # 各フレームの結果を処理\n",
    "            for i, result in enumerate(results):\n",
    "                # 当該フレームに対する検出結果だけをまとめる\n",
    "                new_faces = []\n",
    "                for box in result.boxes.xyxy:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    # リサイズ前の座標に戻す\n",
    "                    x1 = int(x1 * width / new_width)\n",
    "                    y1 = int(y1 * height / new_height)\n",
    "                    x2 = int(x2 * width / new_width)\n",
    "                    y2 = int(y2 * height / new_height)\n",
    "\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    new_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "                # ---------------------------\n",
    "                # 顔領域の補完(トラック更新)\n",
    "                # ---------------------------\n",
    "                face_tracks = update_face_tracks(face_tracks, new_faces, iou_thresh=0.5, max_life=max_life)\n",
    "\n",
    "                # ぼかし処理（顔領域を縮小→ぼかし→拡大）\n",
    "                blur_start_time = time.time()\n",
    "                for (fx1, fy1, fx2, fy2, _) in face_tracks:\n",
    "                    face_roi = original_frames[i][fy1:fy2, fx1:fx2]\n",
    "                    original_frames[i][fy1:fy2, fx1:fx2] = blur_face(face_roi, ksize=(15, 15))\n",
    "                blur_end_time = time.time()\n",
    "                blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "                out.write(original_frames[i])\n",
    "\n",
    "            # 進捗ログ\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s\")\n",
    "                print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "            # バッチをクリア\n",
    "            frame_batch = []\n",
    "            original_frames = []\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - YOLO: 0.182s, Blur: 0.000s\n",
    "Estimated remaining time: 41.6 seconds\n",
    "Frame 200/615 - YOLO: 0.188s, Blur: 0.000s\n",
    "Estimated remaining time: 31.1 seconds\n",
    "Frame 300/615 - YOLO: 0.182s, Blur: 0.000s\n",
    "Estimated remaining time: 23.2 seconds\n",
    "Frame 400/615 - YOLO: 0.183s, Blur: 0.000s\n",
    "Estimated remaining time: 16.0 seconds\n",
    "Frame 500/615 - YOLO: 0.201s, Blur: 0.000s\n",
    "Estimated remaining time: 8.6 seconds\n",
    "\n",
    "Total processing time: 45.1 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画書き出しの並列処理（39.6 seconds）\n",
    "現時点で最高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOv11 Nano（yolov11n-face.pt）を使用\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCV のスレッド数を最適化\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"YOLOのstride=32の倍数になるようリサイズ\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width = (width // 32) * 32 + (32 if width % 32 != 0 else 0)\n",
    "    return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR), new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"顔部分を縮小→GaussionBlur→拡大 でぼかし処理\"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    2つのバウンディングボックス(boxA, boxB)に対するIoU(Intersection over Union)を計算\n",
    "    box = (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=4):\n",
    "    \"\"\"\n",
    "    前フレームまでの顔領域リスト(face_tracks)に、今フレームの検出結果(new_dets)を反映して更新する。\n",
    "    face_tracks: [(x1, y1, x2, y2, life), ... ]\n",
    "    new_dets: [(x1, y1, x2, y2), ... ]\n",
    "    iou_thresh: IoUがこの値以上なら同じ顔とみなす\n",
    "    max_life: ライフ上限(継続保持するフレーム数)\n",
    "    \"\"\"\n",
    "    updated_tracks = []\n",
    "\n",
    "    # 既存トラックのライフを1減らす\n",
    "    for (tx1, ty1, tx2, ty2, life) in face_tracks:\n",
    "        updated_tracks.append([tx1, ty1, tx2, ty2, life - 1])\n",
    "\n",
    "    # new_dets と既存トラックをマッチングしてアップデート\n",
    "    for (nx1, ny1, nx2, ny2) in new_dets:\n",
    "        best_iou = 0\n",
    "        best_index = -1\n",
    "        for i, (tx1, ty1, tx2, ty2, life) in enumerate(updated_tracks):\n",
    "            iou_val = compute_iou((tx1, ty1, tx2, ty2), (nx1, ny1, nx2, ny2))\n",
    "            if iou_val > best_iou:\n",
    "                best_iou = iou_val\n",
    "                best_index = i\n",
    "\n",
    "        if best_iou >= iou_thresh and best_index >= 0:\n",
    "            # 既存のトラックを更新\n",
    "            updated_tracks[best_index][0] = nx1\n",
    "            updated_tracks[best_index][1] = ny1\n",
    "            updated_tracks[best_index][2] = nx2\n",
    "            updated_tracks[best_index][3] = ny2\n",
    "            updated_tracks[best_index][4] = max_life\n",
    "        else:\n",
    "            # 新しいトラックとして追加\n",
    "            updated_tracks.append([nx1, ny1, nx2, ny2, max_life])\n",
    "\n",
    "    # lifeが0以下のトラックを削除\n",
    "    filtered_tracks = []\n",
    "    for t in updated_tracks:\n",
    "        if t[4] > 0:\n",
    "            filtered_tracks.append(t)\n",
    "\n",
    "    return filtered_tracks\n",
    "\n",
    "class FrameWriter(threading.Thread):\n",
    "    \"\"\"\n",
    "    別スレッドでフレームを書き込むクラス\n",
    "    メインスレッドからフレームを queue に put しておき、\n",
    "    このスレッドは queue.get() でフレームを取り出して書き出す。\n",
    "    \"\"\"\n",
    "    def __init__(self, video_writer, frame_queue):\n",
    "        super().__init__()\n",
    "        self.video_writer = video_writer\n",
    "        self.frame_queue = frame_queue\n",
    "        self.stop_signal = False\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            if self.stop_signal and self.frame_queue.empty():\n",
    "                break\n",
    "            try:\n",
    "                # queue からフレームを受け取る\n",
    "                frame = self.frame_queue.get(timeout=0.1)\n",
    "            except:\n",
    "                continue\n",
    "            # VideoWriter に書き込み\n",
    "            self.video_writer.write(frame)\n",
    "            self.frame_queue.task_done()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # 別スレッドで書き込みを実行するための準備\n",
    "    frame_queue = Queue(maxsize=10)  # キューサイズは適宜調整\n",
    "    writer_thread = FrameWriter(out_writer, frame_queue)\n",
    "    writer_thread.start()\n",
    "\n",
    "    frame_count = 0\n",
    "    face_tracks = []  # IOUによる顔領域補完用\n",
    "    start_time = time.time()\n",
    "\n",
    "    frame_batch = []\n",
    "    original_frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        resized_frame, new_width, new_height = resize_to_stride32(frame)\n",
    "        frame_batch.append(resized_frame)\n",
    "        original_frames.append(frame)\n",
    "\n",
    "        # バッチ推論\n",
    "        if len(frame_batch) == batch_size or frame_count == total_frames:\n",
    "            yolo_start_time = time.time()\n",
    "\n",
    "            # テンソル変換\n",
    "            frame_tensors = [\n",
    "                torch.from_numpy(f).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "                for f in frame_batch\n",
    "            ]\n",
    "            batch_tensor = torch.cat(frame_tensors, dim=0)\n",
    "\n",
    "            # 推論\n",
    "            results = model.predict(\n",
    "                batch_tensor,\n",
    "                verbose=False,\n",
    "                imgsz=(new_width, new_height),\n",
    "                conf=0.25,\n",
    "                iou=0.3,\n",
    "                agnostic_nms=True\n",
    "            )\n",
    "\n",
    "            yolo_end_time = time.time()\n",
    "            yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "            # 結果処理\n",
    "            for i, result in enumerate(results):\n",
    "                new_faces = []\n",
    "                for box in result.boxes.xyxy:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    # 座標を元サイズに戻す\n",
    "                    x1 = int(x1 * width / new_width)\n",
    "                    y1 = int(y1 * height / new_height)\n",
    "                    x2 = int(x2 * width / new_width)\n",
    "                    y2 = int(y2 * height / new_height)\n",
    "\n",
    "                    # 領域が不正でなければ取得\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    new_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "                # 顔領域トラック更新 (IOUによる補完)\n",
    "                face_tracks = update_face_tracks(face_tracks, new_faces, iou_thresh=0.5, max_life=4)\n",
    "\n",
    "                # ぼかし処理\n",
    "                blur_start_time = time.time()\n",
    "                for (fx1, fy1, fx2, fy2, _) in face_tracks:\n",
    "                    face = original_frames[i][fy1:fy2, fx1:fx2]\n",
    "                    original_frames[i][fy1:fy2, fx1:fx2] = blur_face(face)\n",
    "                blur_end_time = time.time()\n",
    "                blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "                # 出力用キューにフレームを積む\n",
    "                frame_queue.put(original_frames[i])\n",
    "\n",
    "            # 進捗ログ\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s\")\n",
    "                print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "            frame_batch = []\n",
    "            original_frames = []\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # ラストフレームまで書き出したらスレッド終了\n",
    "    writer_thread.stop()\n",
    "    writer_thread.join()\n",
    "\n",
    "    out_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "process_video(\"input.mp4\", \"output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - YOLO: 0.182s, Blur: 0.000s\n",
    "Estimated remaining time: 37.4 seconds\n",
    "Frame 200/615 - YOLO: 0.186s, Blur: 0.000s\n",
    "Estimated remaining time: 27.6 seconds\n",
    "Frame 300/615 - YOLO: 0.181s, Blur: 0.000s\n",
    "Estimated remaining time: 20.5 seconds\n",
    "Frame 400/615 - YOLO: 0.183s, Blur: 0.000s\n",
    "Estimated remaining time: 14.1 seconds\n",
    "Frame 500/615 - YOLO: 0.199s, Blur: 0.000s\n",
    "Estimated remaining time: 7.6 seconds\n",
    "\n",
    "Total processing time: 39.6 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人物トラッキングとの組み合わせ (フレームスキップ検出)（77.3 seconds）\n",
    "精度が落ち、処理速度も低下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "import math\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# YOLOモデル（例: yolov11n-face.pt）\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCV のスレッド数を最適化\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"YOLOのstride=32の倍数になるようリサイズ\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width = (width // 32) * 32 + (32 if width % 32 != 0 else 0)\n",
    "    return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR), new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"顔部分を縮小→GaussianBlur→拡大 でぼかし処理\"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    2つのバウンディングボックス(boxA, boxB)に対するIoU(Intersection over Union)を計算\n",
    "    box = (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# シンプルなクラス: \"トラッカー + ID + bounding box管理\" をまとめる\n",
    "# -------------------------------------------------------\n",
    "class FaceTracker:\n",
    "    def __init__(self, tracker_id, box, frame):\n",
    "        \"\"\"\n",
    "        tracker_id: ユニークID\n",
    "        box: (x1, y1, x2, y2)\n",
    "        frame: 最初に初期化するフレーム\n",
    "        \"\"\"\n",
    "        self.id = tracker_id\n",
    "\n",
    "        # CSRTトラッカー作成（OpenCVバージョンに応じて変更）\n",
    "        self.tracker = cv2.legacy.TrackerCSRT_create()\n",
    "\n",
    "        # OpenCVトラッカーが要求する形式 (x, y, w, h)\n",
    "        x1, y1, x2, y2 = box\n",
    "        init_box = (x1, y1, x2 - x1, y2 - y1)\n",
    "\n",
    "        # トラッカーを初期化\n",
    "        self.tracker.init(frame, init_box)\n",
    "\n",
    "        # 今のbounding box（最新状態）\n",
    "        self.box = (x1, y1, x2, y2)\n",
    "\n",
    "        # トラッカーが生存しているか\n",
    "        self.active = True\n",
    "\n",
    "    def update(self, frame):\n",
    "        \"\"\"\n",
    "        毎フレーム呼ばれて、トラッカーをアップデート\n",
    "        成功すれば self.box を更新、失敗したら self.active = False\n",
    "        \"\"\"\n",
    "        success, tracked_box = self.tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = tracked_box\n",
    "            x2 = x + w\n",
    "            y2 = y + h\n",
    "            self.box = (int(x), int(y), int(x2), int(y2))\n",
    "        else:\n",
    "            self.active = False\n",
    "\n",
    "\n",
    "def process_video(input_path, output_path, detect_interval=5):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 顔トラッカーの管理用（ID -> FaceTracker）\n",
    "    face_trackers = {}\n",
    "    next_tracker_id = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_count += 1\n",
    "\n",
    "        # ---------------------------\n",
    "        # 1) detect_interval フレームに1回だけ YOLO で検出を行う\n",
    "        # ---------------------------\n",
    "        if frame_count % detect_interval == 1:\n",
    "            # YOLO 推論用にリサイズ\n",
    "            resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "\n",
    "            frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "            # 推論\n",
    "            results = model.predict(frame_tensor, verbose=False, imgsz=(new_w, new_h), conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "            result = results[0]\n",
    "\n",
    "            # 新たに検出された顔のリスト\n",
    "            detected_boxes = []\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                # リサイズ前のスケールに戻す\n",
    "                x1 = int(x1 * width / new_w)\n",
    "                y1 = int(y1 * height / new_h)\n",
    "                x2 = int(x2 * width / new_w)\n",
    "                y2 = int(y2 * height / new_h)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "                if x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                detected_boxes.append((x1, y1, x2, y2))\n",
    "\n",
    "            # ---------------------------\n",
    "            # 2) 現在の face_trackers と new detection を IoU で突き合わせ\n",
    "            # ---------------------------\n",
    "            used_tracker_ids = set()\n",
    "            for dbox in detected_boxes:\n",
    "                x1d, y1d, x2d, y2d = dbox\n",
    "                best_iou = 0\n",
    "                best_id = None\n",
    "\n",
    "                # 既存トラッカーと照合\n",
    "                for t_id, ftracker in face_trackers.items():\n",
    "                    if not ftracker.active:\n",
    "                        continue\n",
    "                    x1t, y1t, x2t, y2t = ftracker.box\n",
    "                    iou_val = compute_iou((x1t, y1t, x2t, y2t), (x1d, y1d, x2d, y2d))\n",
    "                    if iou_val > best_iou:\n",
    "                        best_iou = iou_val\n",
    "                        best_id = t_id\n",
    "\n",
    "                # あるトラッカーと十分重なるなら、そのトラッカーを再初期化(リセット)して使う\n",
    "                if best_iou > 0.3 and best_id is not None:\n",
    "                    used_tracker_ids.add(best_id)\n",
    "                    face_trackers[best_id].tracker.clear()  # 古いトラッカーを破棄\n",
    "                    face_trackers[best_id].tracker = cv2.legacy.TrackerCSRT_create()\n",
    "                    # (x, y, w, h)\n",
    "                    init_box = (x1d, y1d, (x2d - x1d), (y2d - y1d))\n",
    "                    face_trackers[best_id].tracker.init(frame, init_box)\n",
    "                    face_trackers[best_id].box = (x1d, y1d, x2d, y2d)\n",
    "                    face_trackers[best_id].active = True\n",
    "                else:\n",
    "                    # 新規トラッカー作成\n",
    "                    new_tracker = FaceTracker(next_tracker_id, (x1d, y1d, x2d, y2d), frame)\n",
    "                    face_trackers[next_tracker_id] = new_tracker\n",
    "                    used_tracker_ids.add(next_tracker_id)\n",
    "                    next_tracker_id += 1\n",
    "\n",
    "            # ---------------------------\n",
    "            # 3) 新規検出で使われなかったトラッカーを停止 (活性フラグ落とす)\n",
    "            # ---------------------------\n",
    "            for t_id, ftracker in face_trackers.items():\n",
    "                if t_id not in used_tracker_ids:\n",
    "                    # すぐ破棄するか、しばらく生かしておいても良い\n",
    "                    ftracker.active = False\n",
    "\n",
    "        else:\n",
    "            # ---------------------------\n",
    "            # 4) フレームスキップ時は、既存のトラッカーで追跡のみ\n",
    "            # ---------------------------\n",
    "            for t_id, ftracker in face_trackers.items():\n",
    "                if ftracker.active:\n",
    "                    ftracker.update(frame)\n",
    "\n",
    "        # ---------------------------\n",
    "        # 5) ぼかし処理＆書き込み\n",
    "        # ---------------------------\n",
    "        for t_id, ftracker in face_trackers.items():\n",
    "            if ftracker.active:\n",
    "                x1, y1, x2, y2 = ftracker.box\n",
    "                # 座標範囲が有効ならぼかし\n",
    "                x1_clamp = max(0, x1)\n",
    "                y1_clamp = max(0, y1)\n",
    "                x2_clamp = min(width, x2)\n",
    "                y2_clamp = min(height, y2)\n",
    "\n",
    "                if x2_clamp > x1_clamp and y2_clamp > y1_clamp:\n",
    "                    face_roi = frame[y1_clamp:y2_clamp, x1_clamp:x2_clamp]\n",
    "                    frame[y1_clamp:y2_clamp, x1_clamp:x2_clamp] = blur_face(face_roi)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        # 進捗ログ（100フレームごと）\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames}, Est. remaining: {remaining_time:.1f} sec\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "\n",
    "process_video(\"input.mp4\", \"output.mp4\", detect_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615, Est. remaining: 49.1 sec\n",
    "Frame 200/615, Est. remaining: 33.3 sec\n",
    "Frame 300/615, Est. remaining: 25.7 sec\n",
    "Frame 400/615, Est. remaining: 20.8 sec\n",
    "Frame 500/615, Est. remaining: 13.9 sec\n",
    "\n",
    "Total processing time: 77.3 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2フレームに1回だけ顔を検出し、検出した顔領域を最大4フレーム保持してモザイクをかけ続ける（31.5 seconds）\n",
    "早い！ちょっと精度が落ちた気もする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"\n",
    "    画像を YOLO のstride=32 の倍数 (height, width) にリサイズする\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width  = (width  // 32) * 32 + (32 if width  % 32 != 0 else 0)\n",
    "    # もし既に 32 の倍数ならそのまま\n",
    "    if new_width == width and new_height == height:\n",
    "        return image, width, height\n",
    "    # そうでなければリサイズ\n",
    "    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized, new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"\n",
    "    顔部分を縮小してガウシアンぼかしをかけてから元サイズに戻す\n",
    "    \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    2フレームに1回だけYOLOで顔を検出し、\n",
    "    検出した顔領域は4フレームぼかしをかけ続ける (margin付き)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # --- 設定 ---\n",
    "    detect_interval = 2   # 2フレームに1回検出\n",
    "    keep_frames = 4       # 4フレームぼかし続ける\n",
    "    margin = 30           # 顔領域を上下左右に拡大するマージン\n",
    "\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # [ [x1, y1, x2, y2, remain], ... ]\n",
    "    face_memory = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 1) 2フレームに1回だけYOLO推論で顔を検出\n",
    "        # ---------------------------------------------\n",
    "        if frame_count % detect_interval == 0:\n",
    "            # (A) まず stride=32 に合わせてリサイズ\n",
    "            resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "\n",
    "            # (B) テンソル化 (BCHW形式)\n",
    "            #     shapeは (1, 3, new_h, new_w) となる\n",
    "            img_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0)\n",
    "            img_tensor = img_tensor.to(device).float() / 255.0\n",
    "\n",
    "            # (C) 推論\n",
    "            results = model.predict(\n",
    "                img_tensor,\n",
    "                verbose=False,\n",
    "                imgsz=(new_w, new_h),  # リサイズ後のサイズ\n",
    "                conf=0.25, \n",
    "                iou=0.3, \n",
    "                agnostic_nms=True\n",
    "            )\n",
    "            result = results[0]\n",
    "\n",
    "            # (D) 検出ボックスをオリジナル座標に戻して face_memory に追加\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1_r, y1_r, x2_r, y2_r = map(int, box)  # resize後の座標\n",
    "\n",
    "                # オリジナルサイズにスケールを戻す\n",
    "                x1 = int(x1_r * width / new_w)\n",
    "                y1 = int(y1_r * height / new_h)\n",
    "                x2 = int(x2_r * width / new_w)\n",
    "                y2 = int(y2_r * height / new_h)\n",
    "\n",
    "                # margin拡大\n",
    "                x1 -= margin\n",
    "                y1 -= margin\n",
    "                x2 += margin\n",
    "                y2 += margin\n",
    "\n",
    "                # 範囲クリップ\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(width,  x2)\n",
    "                y2 = min(height, y2)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "\n",
    "                # 4フレームぼかし保持\n",
    "                face_memory.append([x1, y1, x2, y2, keep_frames])\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 2) face_memory の領域をぼかし & remain をデクリメント\n",
    "        # ---------------------------------------------\n",
    "        for i, face_box in enumerate(face_memory):\n",
    "            x1, y1, x2, y2, remain = face_box\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "            frame[y1:y2, x1:x2] = blur_face(face_roi)\n",
    "            face_memory[i][4] = remain - 1\n",
    "\n",
    "        # remainが0以下のものを除去\n",
    "        face_memory = [f for f in face_memory if f[4] > 0]\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 3) 書き込み & ログ\n",
    "        # ---------------------------------------------\n",
    "        out.write(frame)\n",
    "\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            if frame_count > 0:\n",
    "                remaining = (elapsed / frame_count) * (total_frames - frame_count)\n",
    "            else:\n",
    "                remaining = 0\n",
    "            print(f\"Frame {frame_count}/{total_frames}, Estimated remaining: {remaining:.1f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "process_video(\"input.mp4\", \"output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615, Estimated remaining: 24.8 seconds\n",
    "Frame 200/615, Estimated remaining: 18.3 seconds\n",
    "Frame 300/615, Estimated remaining: 13.7 seconds\n",
    "Frame 400/615, Estimated remaining: 9.4 seconds\n",
    "Frame 500/615, Estimated remaining: 5.3 seconds\n",
    "\n",
    "Total processing time: 31.5 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP16 / 半精度（32.6 seconds）\n",
    "効果ないように見えたので却下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device).half()\n",
    "\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"\n",
    "    画像を YOLO のstride=32 の倍数 (height, width) にリサイズする\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width  = (width  // 32) * 32 + (32 if width  % 32 != 0 else 0)\n",
    "    # もし既に 32 の倍数ならそのまま\n",
    "    if new_width == width and new_height == height:\n",
    "        return image, width, height\n",
    "    # そうでなければリサイズ\n",
    "    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized, new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"\n",
    "    顔部分を縮小してガウシアンぼかしをかけてから元サイズに戻す\n",
    "    \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    2フレームに1回だけYOLOで顔を検出し、\n",
    "    検出した顔領域は4フレームぼかしをかけ続ける (margin付き)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # --- 設定 ---\n",
    "    detect_interval = 2   # 2フレームに1回検出\n",
    "    keep_frames = 4       # 4フレームぼかし続ける\n",
    "    margin = 30           # 顔領域を上下左右に拡大するマージン\n",
    "\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # [ [x1, y1, x2, y2, remain], ... ]\n",
    "    face_memory = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 1) 2フレームに1回だけYOLO推論で顔を検出\n",
    "        # ---------------------------------------------\n",
    "        if frame_count % detect_interval == 0:\n",
    "            # (A) まず stride=32 に合わせてリサイズ\n",
    "            resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "\n",
    "            # (B) テンソル化 (BCHW形式)\n",
    "            #     shapeは (1, 3, new_h, new_w) となる\n",
    "            img_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0)\n",
    "            img_tensor = img_tensor.to(device).float() / 255.0\n",
    "\n",
    "            # (C) 推論\n",
    "            results = model.predict(\n",
    "                img_tensor,\n",
    "                verbose=False,\n",
    "                imgsz=(new_w, new_h),  # リサイズ後のサイズ\n",
    "                conf=0.25, \n",
    "                iou=0.3, \n",
    "                agnostic_nms=True\n",
    "            )\n",
    "            result = results[0]\n",
    "\n",
    "            # (D) 検出ボックスをオリジナル座標に戻して face_memory に追加\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1_r, y1_r, x2_r, y2_r = map(int, box)  # resize後の座標\n",
    "\n",
    "                # オリジナルサイズにスケールを戻す\n",
    "                x1 = int(x1_r * width / new_w)\n",
    "                y1 = int(y1_r * height / new_h)\n",
    "                x2 = int(x2_r * width / new_w)\n",
    "                y2 = int(y2_r * height / new_h)\n",
    "\n",
    "                # margin拡大\n",
    "                x1 -= margin\n",
    "                y1 -= margin\n",
    "                x2 += margin\n",
    "                y2 += margin\n",
    "\n",
    "                # 範囲クリップ\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(width,  x2)\n",
    "                y2 = min(height, y2)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "\n",
    "                # 4フレームぼかし保持\n",
    "                face_memory.append([x1, y1, x2, y2, keep_frames])\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 2) face_memory の領域をぼかし & remain をデクリメント\n",
    "        # ---------------------------------------------\n",
    "        for i, face_box in enumerate(face_memory):\n",
    "            x1, y1, x2, y2, remain = face_box\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "            frame[y1:y2, x1:x2] = blur_face(face_roi)\n",
    "            face_memory[i][4] = remain - 1\n",
    "\n",
    "        # remainが0以下のものを除去\n",
    "        face_memory = [f for f in face_memory if f[4] > 0]\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 3) 書き込み & ログ\n",
    "        # ---------------------------------------------\n",
    "        out.write(frame)\n",
    "\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            if frame_count > 0:\n",
    "                remaining = (elapsed / frame_count) * (total_frames - frame_count)\n",
    "            else:\n",
    "                remaining = 0\n",
    "            print(f\"Frame {frame_count}/{total_frames}, Estimated remaining: {remaining:.1f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "process_video(\"input.mp4\", \"output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` \n",
    "Frame 100/615, Estimated remaining: 31.0 seconds\n",
    "Frame 200/615, Estimated remaining: 20.8 seconds\n",
    "Frame 300/615, Estimated remaining: 14.9 seconds\n",
    "Frame 400/615, Estimated remaining: 10.0 seconds\n",
    "Frame 500/615, Estimated remaining: 5.5 seconds\n",
    "\n",
    "Total processing time: 32.6 seconds\n",
    "```\n",
    "- モデルに対して、half=True -> 効果なし\n",
    "- .half() -> 効果なし"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結論版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"\n",
    "    画像を YOLO のstride=32 の倍数 (height, width) にリサイズする\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width  = (width  // 32) * 32 + (32 if width  % 32 != 0 else 0)\n",
    "    # もし既に 32 の倍数ならそのまま\n",
    "    if new_width == width and new_height == height:\n",
    "        return image, width, height\n",
    "    # そうでなければリサイズ\n",
    "    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized, new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"\n",
    "    顔部分を縮小してガウシアンぼかしをかけてから元サイズに戻す\n",
    "    \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\"\n",
    "    2フレームに1回だけYOLOで顔を検出し、\n",
    "    検出した顔領域は4フレームぼかしをかけ続ける (margin付き)\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # --- 設定 ---\n",
    "    detect_interval = 2   # 2フレームに1回検出\n",
    "    keep_frames = 4       # 4フレームぼかし続ける\n",
    "    margin = 10           # 顔領域を上下左右に拡大するマージン\n",
    "\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # [ [x1, y1, x2, y2, remain], ... ]\n",
    "    face_memory = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 1) 2フレームに1回だけYOLO推論で顔を検出\n",
    "        # ---------------------------------------------\n",
    "        if frame_count == 1 or frame_count % detect_interval == 0:\n",
    "            # (A) まず stride=32 に合わせてリサイズ\n",
    "            resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "\n",
    "            # (B) テンソル化 (BCHW形式)\n",
    "            #     shapeは (1, 3, new_h, new_w) となる\n",
    "            img_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0)\n",
    "            img_tensor = img_tensor.to(device).float() / 255.0\n",
    "\n",
    "            # (C) 推論\n",
    "            results = model.predict(\n",
    "                img_tensor,\n",
    "                verbose=False,\n",
    "                imgsz=(new_w, new_h),  # リサイズ後のサイズ\n",
    "                conf=0.25, \n",
    "                iou=0.3, \n",
    "                agnostic_nms=True\n",
    "            )\n",
    "            result = results[0]\n",
    "\n",
    "            # (D) 検出ボックスをオリジナル座標に戻して face_memory に追加\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1_r, y1_r, x2_r, y2_r = map(int, box)  # resize後の座標\n",
    "\n",
    "                # オリジナルサイズにスケールを戻す\n",
    "                x1 = int(x1_r * width / new_w)\n",
    "                y1 = int(y1_r * height / new_h)\n",
    "                x2 = int(x2_r * width / new_w)\n",
    "                y2 = int(y2_r * height / new_h)\n",
    "\n",
    "                # margin拡大\n",
    "                x1 -= margin\n",
    "                y1 -= margin\n",
    "                x2 += margin\n",
    "                y2 += margin\n",
    "\n",
    "                # 範囲クリップ\n",
    "                x1 = max(0, x1)\n",
    "                y1 = max(0, y1)\n",
    "                x2 = min(width,  x2)\n",
    "                y2 = min(height, y2)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue\n",
    "\n",
    "                # 4フレームぼかし保持\n",
    "                face_memory.append([x1, y1, x2, y2, keep_frames])\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 2) face_memory の領域をぼかし & remain をデクリメント\n",
    "        # ---------------------------------------------\n",
    "        for i, face_box in enumerate(face_memory):\n",
    "            x1, y1, x2, y2, remain = face_box\n",
    "            face_roi = frame[y1:y2, x1:x2]\n",
    "            frame[y1:y2, x1:x2] = blur_face(face_roi)\n",
    "            face_memory[i][4] = remain - 1\n",
    "\n",
    "        # remainが0以下のものを除去\n",
    "        face_memory = [f for f in face_memory if f[4] > 0]\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 3) 書き込み & ログ\n",
    "        # ---------------------------------------------\n",
    "        out.write(frame)\n",
    "\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            if frame_count > 0:\n",
    "                remaining = (elapsed / frame_count) * (total_frames - frame_count)\n",
    "            else:\n",
    "                remaining = 0\n",
    "            print(f\"Frame {frame_count}/{total_frames}, Estimated remaining: {remaining:.1f} seconds\")\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "process_video(\"input.mp4\", \"output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最終施策（23.1 seconds）\n",
    "- バッチ推論 (batch_size=4)\n",
    "- 2フレームに1回の顔検出\n",
    "- 顔の位置補完 (IOU)\n",
    "- 並列書き出し (FrameWriter クラス)\n",
    "- マージン拡大 or keep_frames 方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) 設定: デバイス・モデル・stride32リサイズ関数\n",
    "# -------------------------------------------------------------\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"\n",
    "    画像を YOLO のstride=32 の倍数 (height, width) にリサイズする\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width  = (width  // 32) * 32 + (32 if width  % 32 != 0 else 0)\n",
    "    if new_width == width and new_height == height:\n",
    "        return image, width, height\n",
    "    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized, new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"\n",
    "    顔部分を縮小してガウシアンぼかしをかけてから元サイズに戻す\n",
    "    \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) 顔の位置補完 (IOU管理)\n",
    "#    - face_tracks: [(x1, y1, x2, y2, life), ...]\n",
    "# -------------------------------------------------------------\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    2つのバウンディングボックス(boxA, boxB)に対するIoU(Intersection over Union)を計算\n",
    "    box = (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=4):\n",
    "    \"\"\"\n",
    "    前フレームまでの顔領域リスト(face_tracks)に、今フレームの検出結果(new_dets)を反映して更新する。\n",
    "      - face_tracks: [(x1, y1, x2, y2, life), ... ]\n",
    "      - new_dets:    [(x1, y1, x2, y2), ... ]\n",
    "    iou_thresh: IoU がこの値以上なら同じ顔とみなす\n",
    "    max_life:   ライフ上限(継続保持するフレーム数)\n",
    "    \"\"\"\n",
    "    updated_tracks = []\n",
    "\n",
    "    # 既存トラックのライフを1減らして用意\n",
    "    for (tx1, ty1, tx2, ty2, life) in face_tracks:\n",
    "        updated_tracks.append([tx1, ty1, tx2, ty2, life - 1])\n",
    "\n",
    "    # new_dets と既存トラックをマッチングしてアップデート\n",
    "    for (nx1, ny1, nx2, ny2) in new_dets:\n",
    "        best_iou = 0\n",
    "        best_index = -1\n",
    "        for i, (tx1, ty1, tx2, ty2, life) in enumerate(updated_tracks):\n",
    "            iou_val = compute_iou((tx1, ty1, tx2, ty2), (nx1, ny1, nx2, ny2))\n",
    "            if iou_val > best_iou:\n",
    "                best_iou = iou_val\n",
    "                best_index = i\n",
    "\n",
    "        if best_iou >= iou_thresh and best_index >= 0:\n",
    "            # 既存のトラックを更新\n",
    "            updated_tracks[best_index][0] = nx1\n",
    "            updated_tracks[best_index][1] = ny1\n",
    "            updated_tracks[best_index][2] = nx2\n",
    "            updated_tracks[best_index][3] = ny2\n",
    "            updated_tracks[best_index][4] = max_life\n",
    "        else:\n",
    "            # 新しいトラックとして追加\n",
    "            updated_tracks.append([nx1, ny1, nx2, ny2, max_life])\n",
    "\n",
    "    # lifeが0以下のトラックを削除\n",
    "    filtered_tracks = []\n",
    "    for t in updated_tracks:\n",
    "        if t[4] > 0:\n",
    "            filtered_tracks.append(t)\n",
    "\n",
    "    return filtered_tracks\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) 動画書き出しを並列化するクラス\n",
    "# -------------------------------------------------------------\n",
    "class FrameWriter(threading.Thread):\n",
    "    \"\"\"\n",
    "    別スレッドでフレームを書き込む。\n",
    "      - メインスレッドでフレームを queue に put する\n",
    "      - ここで queue.get() して VideoWriter.write() する\n",
    "    \"\"\"\n",
    "    def __init__(self, video_writer, frame_queue):\n",
    "        super().__init__()\n",
    "        self.video_writer = video_writer\n",
    "        self.frame_queue = frame_queue\n",
    "        self.stop_signal = False\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            if self.stop_signal and self.frame_queue.empty():\n",
    "                break\n",
    "            try:\n",
    "                frame = self.frame_queue.get(timeout=0.1)\n",
    "            except:\n",
    "                continue\n",
    "            self.video_writer.write(frame)\n",
    "            self.frame_queue.task_done()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) メイン処理: バッチ推論 + 2フレームに1回検出 + IOU補完 + 並列書き出し\n",
    "# -------------------------------------------------------------\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    \"\"\"\n",
    "    - バッチ推論: frame_batch に複数フレームを貯めてまとめてYOLO\n",
    "    - 2フレームに1回 (detect_interval=2) のみ「検出」対象として推論\n",
    "    - 顔の位置補完(IOU)\n",
    "    - 並列でVideoWriter書き込み\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # 別スレッドで書き出し\n",
    "    frame_queue = Queue(maxsize=10)\n",
    "    writer_thread = FrameWriter(out_writer, frame_queue)\n",
    "    writer_thread.start()\n",
    "\n",
    "    detect_interval = 2\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 顔トラックをIOUで補完\n",
    "    face_tracks = []\n",
    "    max_life = 4\n",
    "\n",
    "    # バッチ用\n",
    "    frame_batch = []       # リサイズ済みフレーム (推論用)\n",
    "    original_frames = []   # 元のフレーム (ぼかし用)\n",
    "    detect_flags = []      # このフレームで検出するか否か (True/False)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # (1) フレームをバッチに追加\n",
    "        resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "        frame_batch.append(resized_frame)\n",
    "        original_frames.append(frame)\n",
    "\n",
    "        # \"2フレームに1回\" で検出\n",
    "        detect_flags.append(frame_count == 1 or (frame_count % detect_interval == 0))\n",
    "\n",
    "        # (2) バッチがいっぱい or 最終フレーム\n",
    "        if len(frame_batch) == batch_size or frame_count == total_frames:\n",
    "            # ------ YOLO 推論 ------\n",
    "            # sub-batch: detect_flags が True のフレームだけまとめて推論する\n",
    "            sub_tensors = []\n",
    "            sub_indices = []\n",
    "            for i, (f, flag) in enumerate(zip(frame_batch, detect_flags)):\n",
    "                if flag:  # このフレームで検出実施\n",
    "                    tensor = torch.from_numpy(f).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "                    sub_tensors.append(tensor)\n",
    "                    sub_indices.append(i)\n",
    "\n",
    "            # sub_tensors をまとめて推論\n",
    "            if len(sub_tensors) > 0:\n",
    "                batch_tensor = torch.cat(sub_tensors, dim=0)\n",
    "                results = model.predict(\n",
    "                    batch_tensor, \n",
    "                    verbose=False,\n",
    "                    imgsz=(new_w, new_h),\n",
    "                    conf=0.25,\n",
    "                    iou=0.3,\n",
    "                    agnostic_nms=True\n",
    "                )\n",
    "            else:\n",
    "                results = []\n",
    "\n",
    "            # ------ 結果をフレームごとに整理 ------\n",
    "            # sub_indices と results[i] を対応付ける\n",
    "            #  => detection_results[フレームバッチ内index] = [(x1,y1,x2,y2), ...]\n",
    "            detection_results = [[] for _ in range(len(frame_batch))]\n",
    "            for r_i, r in enumerate(results):\n",
    "                i_batch_index = sub_indices[r_i]\n",
    "                new_faces = []\n",
    "                for box in r.boxes.xyxy:\n",
    "                    x1_r, y1_r, x2_r, y2_r = map(int, box)\n",
    "                    # オリジナル座標に戻す\n",
    "                    x1 = int(x1_r * width / new_w)\n",
    "                    y1 = int(y1_r * height / new_h)\n",
    "                    x2 = int(x2_r * width / new_w)\n",
    "                    y2 = int(y2_r * height / new_h)\n",
    "                    # 有効範囲チェック\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    new_faces.append((x1, y1, x2, y2))\n",
    "                detection_results[i_batch_index] = new_faces\n",
    "\n",
    "            # ------ 各フレームに対して IOU 補完 & ぼかし ------\n",
    "            # バッチ内フレームを順に処理\n",
    "            for i in range(len(frame_batch)):\n",
    "                # 新規検出があればトラック更新\n",
    "                new_dets = detection_results[i] if i < len(detection_results) else []\n",
    "                face_tracks = update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=max_life)\n",
    "\n",
    "                # 顔トラックをぼかし\n",
    "                for (fx1, fy1, fx2, fy2, _) in face_tracks:\n",
    "                    face_roi = original_frames[i][fy1:fy2, fx1:fx2]\n",
    "                    original_frames[i][fy1:fy2, fx1:fx2] = blur_face(face_roi)\n",
    "\n",
    "                # 出力キューにフレームを詰める\n",
    "                frame_queue.put(original_frames[i])\n",
    "\n",
    "            # 進捗ログ\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                remaining = (elapsed / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames}, Estimated remaining: {remaining:.1f} sec\")\n",
    "\n",
    "            # バッチをクリア\n",
    "            frame_batch = []\n",
    "            original_frames = []\n",
    "            detect_flags = []\n",
    "\n",
    "    # 終了処理\n",
    "    cap.release()\n",
    "\n",
    "    # フレーム書き出しスレッド終了\n",
    "    writer_thread.stop()\n",
    "    writer_thread.join()\n",
    "    out_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) 実行例\n",
    "# -------------------------------------------------------------\n",
    "process_video(\"input.mp4\", \"output.mp4\", batch_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615, Estimated remaining: 24.7 sec\n",
    "Frame 200/615, Estimated remaining: 17.2 sec\n",
    "Frame 300/615, Estimated remaining: 12.4 sec\n",
    "Frame 400/615, Estimated remaining: 8.4 sec\n",
    "Frame 500/615, Estimated remaining: 4.5 sec\n",
    "\n",
    "Total processing time: 23.1 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- LUTの適用\n",
    "- 音声の保持\n",
    "- 回転した動画の確認\n",
    "- 結局、opencv-pythonでいいのでは？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
