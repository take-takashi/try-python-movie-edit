{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改善案（Total processing time: 54.7 seconds）\n",
    "\n",
    "|改善ポイント|方法|効果|\n",
    "|---|---|---|\n",
    "|YOLOの前処理を最適化|torch.from_numpy() 削減|CPU負荷軽減（メモリオーバーヘッド削減）|\n",
    "|YOLOのバッチ処理|5フレームごとに推論|YOLO推論回数を1/5に削減（高速化）|\n",
    "|並列処理|ThreadPoolExecutor を使用|CPUとGPUの同時活用（処理効率UP）|\n",
    "|動画のエンコード最適化|H.264 コーデックを使用|動画ファイルの圧縮効率UP・書き出し高速化|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(25, 25)):\n",
    "    \"\"\" ガウシアンぼかし処理（高速化） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 15) if image.size > 0 else image\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速化を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=4)  # **モザイクを4フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO の入力サイズを 640x640 に変更**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 640)\n",
    "                y1 = int(y1 * height / 640)\n",
    "                x2 = int(x2 * width / 640)\n",
    "                y2 = int(y2 * height / 640)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（カーネルサイズを小さく）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(25, 25))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - Resize: 0.003s, YOLO: 0.023s, Blur: 0.021s, Total: 0.069s\n",
    "Estimated remaining time: 46.6 seconds\n",
    "Frame 200/615 - Resize: 0.002s, YOLO: 0.022s, Blur: 0.021s, Total: 0.070s\n",
    "Estimated remaining time: 37.0 seconds\n",
    "Frame 300/615 - Resize: 0.003s, YOLO: 0.036s, Blur: 0.023s, Total: 0.087s\n",
    "Estimated remaining time: 28.6 seconds\n",
    "Frame 400/615 - Resize: 0.002s, YOLO: 0.019s, Blur: 0.022s, Total: 0.070s\n",
    "Estimated remaining time: 19.6 seconds\n",
    "Frame 500/615 - Resize: 0.002s, YOLO: 0.039s, Blur: 0.025s, Total: 0.097s\n",
    "Estimated remaining time: 10.5 seconds\n",
    "\n",
    "Total processing time: 54.7 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改善のポイント（Total processing time: 58.0 seconds）\n",
    "- ✅ YOLOの内部処理を活かし、推論高速化\n",
    "- ✅ MPSのメモリ管理を適切化\n",
    "- ✅ フレームの読み込みを非同期化し、ボトルネック解消\n",
    "- ✅ GaussianBlurを並列化して処理時間短縮\n",
    "\n",
    "結論：微妙だった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolo11n.pt\").to(device)\n",
    "\n",
    "# OpenCV の並列処理を有効化\n",
    "cv2.setNumThreads(4)\n",
    "\n",
    "def blur_face(image, ksize=(25, 25)):\n",
    "    \"\"\" ガウシアンぼかし処理（高速化） \"\"\"\n",
    "    return cv2.GaussianBlur(image, ksize, 15) if image.size > 0 else image\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速化を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=4)  # **モザイクを4フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLO の入力サイズを 640x640 に変更**\n",
    "        resize_start_time = time.time()\n",
    "        resized_frame = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_AREA)\n",
    "        resize_end_time = time.time()\n",
    "        resize_processing_time = resize_end_time - resize_start_time\n",
    "\n",
    "        # **YOLOの推論**\n",
    "        yolo_start_time = time.time()\n",
    "        torch.mps.empty_cache()  # **MPSのメモリ管理を最適化**\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                x1 = int(x1 * width / 640)\n",
    "                y1 = int(y1 * height / 640)\n",
    "                x2 = int(x2 * width / 640)\n",
    "                y2 = int(y2 * height / 640)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（カーネルサイズを小さく）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(25, 25))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - Resize: {resize_processing_time:.3f}s, YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - Resize: 0.002s, YOLO: 0.023s, Blur: 0.020s, Total: 0.068s\n",
    "Estimated remaining time: 54.2 seconds\n",
    "Frame 200/615 - Resize: 0.002s, YOLO: 0.025s, Blur: 0.021s, Total: 0.073s\n",
    "Estimated remaining time: 41.7 seconds\n",
    "Frame 300/615 - Resize: 0.002s, YOLO: 0.029s, Blur: 0.023s, Total: 0.079s\n",
    "Estimated remaining time: 31.5 seconds\n",
    "Frame 400/615 - Resize: 0.002s, YOLO: 0.032s, Blur: 0.021s, Total: 0.082s\n",
    "Estimated remaining time: 21.2 seconds\n",
    "Frame 500/615 - Resize: 0.002s, YOLO: 0.037s, Blur: 0.022s, Total: 0.088s\n",
    "Estimated remaining time: 11.2 seconds\n",
    "\n",
    "Total processing time: 58.0 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結局、顔検出のみにする（Total processing time: 84.3 seconds）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# MacBook Air M3 の GPU（Metal MPS）を活用\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# **YOLOv11 Nano（yolo11n.pt）を使用**\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "# OpenCV のスレッド数を最適化\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"YOLOのstride=32の倍数になるようリサイズ\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width = (width // 32) * 32 + (32 if width % 32 != 0 else 0)\n",
    "    return cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR), new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\" 顔部分のサイズを縮小してぼかしをかけてから元サイズに戻す \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, (ksize[0], ksize[1]), interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def process_video(input_path, output_path):\n",
    "    \"\"\" 動画を処理し、毎フレーム顔を検出しながら、高速化を実施 \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "\n",
    "    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    face_memory = deque(maxlen=4)  # **モザイクを4フレームまで保持**\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        frame_start_time = time.time()\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        current_faces = []\n",
    "\n",
    "        # **YOLOの推論（stride 32 の倍数にリサイズ）**\n",
    "        yolo_start_time = time.time()\n",
    "        \n",
    "        if frame_count % 50 == 0:\n",
    "            torch.mps.empty_cache()  # **50フレームごとにキャッシュクリア**\n",
    "\n",
    "        resized_frame, new_width, new_height = resize_to_stride32(frame)\n",
    "\n",
    "        frame_tensor = torch.from_numpy(resized_frame).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "        results = model.predict(frame_tensor, verbose=False, imgsz=(new_height, new_width), conf=0.25, iou=0.3, agnostic_nms=True)\n",
    "        \n",
    "        yolo_end_time = time.time()\n",
    "        yolo_processing_time = yolo_end_time - yolo_start_time\n",
    "\n",
    "        for result in results:\n",
    "            for box in result.boxes.xyxy:\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                # リサイズ前の座標に戻す\n",
    "                x1 = int(x1 * width / new_width)\n",
    "                y1 = int(y1 * height / new_height)\n",
    "                x2 = int(x2 * width / new_width)\n",
    "                y2 = int(y2 * height / new_height)\n",
    "\n",
    "                if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                    continue\n",
    "                current_faces.append((x1, y1, x2, y2))\n",
    "\n",
    "        face_memory.append(current_faces)\n",
    "\n",
    "        # **ぼかし処理（顔領域を縮小→ぼかし→拡大）**\n",
    "        blur_start_time = time.time()\n",
    "        for faces in face_memory:\n",
    "            for (x1, y1, x2, y2) in faces:\n",
    "                face = frame[y1:y2, x1:x2]\n",
    "                frame[y1:y2, x1:x2] = blur_face(face, ksize=(15, 15))\n",
    "        blur_end_time = time.time()\n",
    "        blur_processing_time = blur_end_time - blur_start_time\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        frame_end_time = time.time()\n",
    "        total_frame_time = frame_end_time - frame_start_time\n",
    "\n",
    "        # **100フレームごとにログを出力**\n",
    "        if frame_count % 100 == 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = (elapsed_time / frame_count) * (total_frames - frame_count)\n",
    "            print(f\"Frame {frame_count}/{total_frames} - YOLO: {yolo_processing_time:.3f}s, Blur: {blur_processing_time:.3f}s, Total: {total_frame_time:.3f}s\")\n",
    "            print(f\"Estimated remaining time: {remaining_time:.1f} seconds\")\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# 動画処理の実行\n",
    "process_video(\"input.mp4\", \"output.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615 - YOLO: 0.046s, Blur: 0.000s, Total: 0.064s\n",
    "Estimated remaining time: 51.5 seconds\n",
    "Frame 200/615 - YOLO: 0.049s, Blur: 0.000s, Total: 0.068s\n",
    "Estimated remaining time: 37.4 seconds\n",
    "Frame 300/615 - YOLO: 0.119s, Blur: 0.001s, Total: 0.153s\n",
    "Estimated remaining time: 33.0 seconds\n",
    "Frame 400/615 - YOLO: 0.119s, Blur: 0.001s, Total: 0.153s\n",
    "Estimated remaining time: 26.2 seconds\n",
    "Frame 500/615 - YOLO: 0.130s, Blur: 0.001s, Total: 0.163s\n",
    "Estimated remaining time: 15.3 seconds\n",
    "\n",
    "Total processing time: 84.3 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結局顔のみ検出の方が精度良い  \n",
    "改善を1個ずつ試していこう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 改めて、最後のソースに対して改善案を出してもらおう"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
