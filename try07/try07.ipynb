{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ここまでのソース（23.0 seconds）\n",
    "- バッチ推論 (batch_size=4)\n",
    "- 2フレームに1回の顔検出\n",
    "- 顔の位置補完 (IOU)\n",
    "- 並列書き出し (FrameWriter クラス)\n",
    "- マージン拡大 or keep_frames 方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from queue import Queue\n",
    "import threading\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) 設定: デバイス・モデル・stride32リサイズ関数\n",
    "# -------------------------------------------------------------\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"\n",
    "    画像を YOLO のstride=32 の倍数 (height, width) にリサイズする\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width  = (width  // 32) * 32 + (32 if width  % 32 != 0 else 0)\n",
    "    if new_width == width and new_height == height:\n",
    "        return image, width, height\n",
    "    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized, new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"\n",
    "    顔部分を縮小してガウシアンぼかしをかけてから元サイズに戻す\n",
    "    \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) 顔の位置補完 (IOU管理)\n",
    "#    - face_tracks: [(x1, y1, x2, y2, life), ...]\n",
    "# -------------------------------------------------------------\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    2つのバウンディングボックス(boxA, boxB)に対するIoU(Intersection over Union)を計算\n",
    "    box = (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=4):\n",
    "    \"\"\"\n",
    "    前フレームまでの顔領域リスト(face_tracks)に、今フレームの検出結果(new_dets)を反映して更新する。\n",
    "      - face_tracks: [(x1, y1, x2, y2, life), ... ]\n",
    "      - new_dets:    [(x1, y1, x2, y2), ... ]\n",
    "    iou_thresh: IoU がこの値以上なら同じ顔とみなす\n",
    "    max_life:   ライフ上限(継続保持するフレーム数)\n",
    "    \"\"\"\n",
    "    updated_tracks = []\n",
    "\n",
    "    # 既存トラックのライフを1減らして用意\n",
    "    for (tx1, ty1, tx2, ty2, life) in face_tracks:\n",
    "        updated_tracks.append([tx1, ty1, tx2, ty2, life - 1])\n",
    "\n",
    "    # new_dets と既存トラックをマッチングしてアップデート\n",
    "    for (nx1, ny1, nx2, ny2) in new_dets:\n",
    "        best_iou = 0\n",
    "        best_index = -1\n",
    "        for i, (tx1, ty1, tx2, ty2, life) in enumerate(updated_tracks):\n",
    "            iou_val = compute_iou((tx1, ty1, tx2, ty2), (nx1, ny1, nx2, ny2))\n",
    "            if iou_val > best_iou:\n",
    "                best_iou = iou_val\n",
    "                best_index = i\n",
    "\n",
    "        if best_iou >= iou_thresh and best_index >= 0:\n",
    "            # 既存のトラックを更新\n",
    "            updated_tracks[best_index][0] = nx1\n",
    "            updated_tracks[best_index][1] = ny1\n",
    "            updated_tracks[best_index][2] = nx2\n",
    "            updated_tracks[best_index][3] = ny2\n",
    "            updated_tracks[best_index][4] = max_life\n",
    "        else:\n",
    "            # 新しいトラックとして追加\n",
    "            updated_tracks.append([nx1, ny1, nx2, ny2, max_life])\n",
    "\n",
    "    # lifeが0以下のトラックを削除\n",
    "    filtered_tracks = []\n",
    "    for t in updated_tracks:\n",
    "        if t[4] > 0:\n",
    "            filtered_tracks.append(t)\n",
    "\n",
    "    return filtered_tracks\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) 動画書き出しを並列化するクラス\n",
    "# -------------------------------------------------------------\n",
    "class FrameWriter(threading.Thread):\n",
    "    \"\"\"\n",
    "    別スレッドでフレームを書き込む。\n",
    "      - メインスレッドでフレームを queue に put する\n",
    "      - ここで queue.get() して VideoWriter.write() する\n",
    "    \"\"\"\n",
    "    def __init__(self, video_writer, frame_queue):\n",
    "        super().__init__()\n",
    "        self.video_writer = video_writer\n",
    "        self.frame_queue = frame_queue\n",
    "        self.stop_signal = False\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            if self.stop_signal and self.frame_queue.empty():\n",
    "                break\n",
    "            try:\n",
    "                frame = self.frame_queue.get(timeout=0.1)\n",
    "            except:\n",
    "                continue\n",
    "            self.video_writer.write(frame)\n",
    "            self.frame_queue.task_done()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) メイン処理: バッチ推論 + 2フレームに1回検出 + IOU補完 + 並列書き出し\n",
    "# -------------------------------------------------------------\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    \"\"\"\n",
    "    - バッチ推論: frame_batch に複数フレームを貯めてまとめてYOLO\n",
    "    - 2フレームに1回 (detect_interval=2) のみ「検出」対象として推論\n",
    "    - 顔の位置補完(IOU)\n",
    "    - 並列でVideoWriter書き込み\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # 別スレッドで書き出し\n",
    "    frame_queue = Queue(maxsize=10)\n",
    "    writer_thread = FrameWriter(out_writer, frame_queue)\n",
    "    writer_thread.start()\n",
    "\n",
    "    detect_interval = 2\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 顔トラックをIOUで補完\n",
    "    face_tracks = []\n",
    "    max_life = 4\n",
    "\n",
    "    # バッチ用\n",
    "    frame_batch = []       # リサイズ済みフレーム (推論用)\n",
    "    original_frames = []   # 元のフレーム (ぼかし用)\n",
    "    detect_flags = []      # このフレームで検出するか否か (True/False)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # (1) フレームをバッチに追加\n",
    "        resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "        frame_batch.append(resized_frame)\n",
    "        original_frames.append(frame)\n",
    "\n",
    "        # \"2フレームに1回\" で検出\n",
    "        detect_flags.append(frame_count == 1 or (frame_count % detect_interval == 0))\n",
    "\n",
    "        # (2) バッチがいっぱい or 最終フレーム\n",
    "        if len(frame_batch) == batch_size or frame_count == total_frames:\n",
    "            # ------ YOLO 推論 ------\n",
    "            # sub-batch: detect_flags が True のフレームだけまとめて推論する\n",
    "            sub_tensors = []\n",
    "            sub_indices = []\n",
    "            for i, (f, flag) in enumerate(zip(frame_batch, detect_flags)):\n",
    "                if flag:  # このフレームで検出実施\n",
    "                    tensor = torch.from_numpy(f).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "                    sub_tensors.append(tensor)\n",
    "                    sub_indices.append(i)\n",
    "\n",
    "            # sub_tensors をまとめて推論\n",
    "            if len(sub_tensors) > 0:\n",
    "                batch_tensor = torch.cat(sub_tensors, dim=0)\n",
    "                results = model.predict(\n",
    "                    batch_tensor, \n",
    "                    verbose=False,\n",
    "                    imgsz=(new_w, new_h),\n",
    "                    conf=0.25,\n",
    "                    iou=0.3,\n",
    "                    agnostic_nms=True\n",
    "                )\n",
    "            else:\n",
    "                results = []\n",
    "\n",
    "            # ------ 結果をフレームごとに整理 ------\n",
    "            # sub_indices と results[i] を対応付ける\n",
    "            #  => detection_results[フレームバッチ内index] = [(x1,y1,x2,y2), ...]\n",
    "            detection_results = [[] for _ in range(len(frame_batch))]\n",
    "            for r_i, r in enumerate(results):\n",
    "                i_batch_index = sub_indices[r_i]\n",
    "                new_faces = []\n",
    "                for box in r.boxes.xyxy:\n",
    "                    x1_r, y1_r, x2_r, y2_r = map(int, box)\n",
    "                    # オリジナル座標に戻す\n",
    "                    x1 = int(x1_r * width / new_w)\n",
    "                    y1 = int(y1_r * height / new_h)\n",
    "                    x2 = int(x2_r * width / new_w)\n",
    "                    y2 = int(y2_r * height / new_h)\n",
    "                    # 有効範囲チェック\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    new_faces.append((x1, y1, x2, y2))\n",
    "                detection_results[i_batch_index] = new_faces\n",
    "\n",
    "            # ------ 各フレームに対して IOU 補完 & ぼかし ------\n",
    "            # バッチ内フレームを順に処理\n",
    "            for i in range(len(frame_batch)):\n",
    "                # 新規検出があればトラック更新\n",
    "                new_dets = detection_results[i] if i < len(detection_results) else []\n",
    "                face_tracks = update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=max_life)\n",
    "\n",
    "                # 顔トラックをぼかし\n",
    "                for (fx1, fy1, fx2, fy2, _) in face_tracks:\n",
    "                    face_roi = original_frames[i][fy1:fy2, fx1:fx2]\n",
    "                    original_frames[i][fy1:fy2, fx1:fx2] = blur_face(face_roi)\n",
    "\n",
    "                # 出力キューにフレームを詰める\n",
    "                frame_queue.put(original_frames[i])\n",
    "\n",
    "            # 進捗ログ\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                remaining = (elapsed / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames}, Estimated remaining: {remaining:.1f} sec\")\n",
    "\n",
    "            # バッチをクリア\n",
    "            frame_batch = []\n",
    "            original_frames = []\n",
    "            detect_flags = []\n",
    "\n",
    "    # 終了処理\n",
    "    cap.release()\n",
    "\n",
    "    # フレーム書き出しスレッド終了\n",
    "    writer_thread.stop()\n",
    "    writer_thread.join()\n",
    "    out_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) 実行例\n",
    "# -------------------------------------------------------------\n",
    "process_video(\"input.mp4\", \"output.mp4\", batch_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1回目（初回）の起動\n",
    "\n",
    "```\n",
    "Frame 100/615, Estimated remaining: 62.4 sec\n",
    "Frame 200/615, Estimated remaining: 33.2 sec\n",
    "Frame 300/615, Estimated remaining: 20.6 sec\n",
    "Frame 400/615, Estimated remaining: 12.8 sec\n",
    "Frame 500/615, Estimated remaining: 6.5 sec\n",
    "\n",
    "Total processing time: 32.6 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2回目の起動\n",
    "```\n",
    "Frame 100/615, Estimated remaining: 22.7 sec\n",
    "Frame 200/615, Estimated remaining: 16.4 sec\n",
    "Frame 300/615, Estimated remaining: 12.0 sec\n",
    "Frame 400/615, Estimated remaining: 8.2 sec\n",
    "Frame 500/615, Estimated remaining: 4.4 sec\n",
    "\n",
    "Total processing time: 23.0 seconds\n",
    "```\n",
    "\n",
    "メモ： `opencv-python` と `opencv-contrib-python` で実行速度の違いはない模様。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LUTの適用（ダメ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from queue import Queue\n",
    "import threading\n",
    "import scipy.interpolate\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) 設定: デバイス・モデル・stride32リサイズ関数\n",
    "# -------------------------------------------------------------\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def load_cube_file(file_path):\n",
    "    lut = []\n",
    "    size = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#') or line == '':\n",
    "                continue\n",
    "            if line.lower().startswith(\"lut_3d_size\"):\n",
    "                size = int(line.split()[-1])\n",
    "            elif line[0].isalpha():\n",
    "                continue\n",
    "            else:\n",
    "                # 各行にRGB値が3つある前提\n",
    "                rgb = list(map(float, line.split()))\n",
    "                if len(rgb) == 3:\n",
    "                    lut.append(rgb)\n",
    "    lut = np.array(lut)\n",
    "    # LUTサイズが明示されていない場合は、立方体の辺の長さを計算\n",
    "    if size is None:\n",
    "        size = int(round(lut.shape[0] ** (1/3)))\n",
    "    lut = lut.reshape((size, size, size, 3))\n",
    "    return lut\n",
    "\n",
    "def apply_3d_lut(image, lut):\n",
    "    # 画像を[0,1]に正規化\n",
    "    image_norm = image.astype(np.float32) / 255.0\n",
    "    size = lut.shape[0]\n",
    "    # LUTの各軸のグリッドを生成（例: 0～1の範囲をsize等分）\n",
    "    grid = np.linspace(0, 1, size)\n",
    "    # 画像を2次元配列（N,3）に変換\n",
    "    flat = image_norm.reshape(-1, 3)\n",
    "    # 各ピクセルに対してLUTから補間\n",
    "    new_flat = scipy.interpolate.interpn((grid, grid, grid), lut, flat, bounds_error=False, fill_value=None)\n",
    "    new_image = new_flat.reshape(image_norm.shape)\n",
    "    # 出力は0～255のuint8画像に戻す\n",
    "    return np.clip(new_image * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"\n",
    "    画像を YOLO のstride=32 の倍数 (height, width) にリサイズする\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width  = (width  // 32) * 32 + (32 if width  % 32 != 0 else 0)\n",
    "    if new_width == width and new_height == height:\n",
    "        return image, width, height\n",
    "    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized, new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"\n",
    "    顔部分を縮小してガウシアンぼかしをかけてから元サイズに戻す\n",
    "    \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) 顔の位置補完 (IOU管理)\n",
    "#    - face_tracks: [(x1, y1, x2, y2, life), ...]\n",
    "# -------------------------------------------------------------\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    2つのバウンディングボックス(boxA, boxB)に対するIoU(Intersection over Union)を計算\n",
    "    box = (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=4):\n",
    "    \"\"\"\n",
    "    前フレームまでの顔領域リスト(face_tracks)に、今フレームの検出結果(new_dets)を反映して更新する。\n",
    "      - face_tracks: [(x1, y1, x2, y2, life), ... ]\n",
    "      - new_dets:    [(x1, y1, x2, y2), ... ]\n",
    "    iou_thresh: IoU がこの値以上なら同じ顔とみなす\n",
    "    max_life:   ライフ上限(継続保持するフレーム数)\n",
    "    \"\"\"\n",
    "    updated_tracks = []\n",
    "\n",
    "    # 既存トラックのライフを1減らして用意\n",
    "    for (tx1, ty1, tx2, ty2, life) in face_tracks:\n",
    "        updated_tracks.append([tx1, ty1, tx2, ty2, life - 1])\n",
    "\n",
    "    # new_dets と既存トラックをマッチングしてアップデート\n",
    "    for (nx1, ny1, nx2, ny2) in new_dets:\n",
    "        best_iou = 0\n",
    "        best_index = -1\n",
    "        for i, (tx1, ty1, tx2, ty2, life) in enumerate(updated_tracks):\n",
    "            iou_val = compute_iou((tx1, ty1, tx2, ty2), (nx1, ny1, nx2, ny2))\n",
    "            if iou_val > best_iou:\n",
    "                best_iou = iou_val\n",
    "                best_index = i\n",
    "\n",
    "        if best_iou >= iou_thresh and best_index >= 0:\n",
    "            # 既存のトラックを更新\n",
    "            updated_tracks[best_index][0] = nx1\n",
    "            updated_tracks[best_index][1] = ny1\n",
    "            updated_tracks[best_index][2] = nx2\n",
    "            updated_tracks[best_index][3] = ny2\n",
    "            updated_tracks[best_index][4] = max_life\n",
    "        else:\n",
    "            # 新しいトラックとして追加\n",
    "            updated_tracks.append([nx1, ny1, nx2, ny2, max_life])\n",
    "\n",
    "    # lifeが0以下のトラックを削除\n",
    "    filtered_tracks = []\n",
    "    for t in updated_tracks:\n",
    "        if t[4] > 0:\n",
    "            filtered_tracks.append(t)\n",
    "\n",
    "    return filtered_tracks\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) 動画書き出しを並列化するクラス\n",
    "# -------------------------------------------------------------\n",
    "class FrameWriter(threading.Thread):\n",
    "    \"\"\"\n",
    "    別スレッドでフレームを書き込む。\n",
    "      - メインスレッドでフレームを queue に put する\n",
    "      - ここで queue.get() して VideoWriter.write() する\n",
    "    \"\"\"\n",
    "    def __init__(self, video_writer, frame_queue):\n",
    "        super().__init__()\n",
    "        self.video_writer = video_writer\n",
    "        self.frame_queue = frame_queue\n",
    "        self.stop_signal = False\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            if self.stop_signal and self.frame_queue.empty():\n",
    "                break\n",
    "            try:\n",
    "                frame = self.frame_queue.get(timeout=0.1)\n",
    "            except:\n",
    "                continue\n",
    "            self.video_writer.write(frame)\n",
    "            self.frame_queue.task_done()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) メイン処理: バッチ推論 + 2フレームに1回検出 + IOU補完 + 並列書き出し\n",
    "# -------------------------------------------------------------\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    \"\"\"\n",
    "    - バッチ推論: frame_batch に複数フレームを貯めてまとめてYOLO\n",
    "    - 2フレームに1回 (detect_interval=2) のみ「検出」対象として推論\n",
    "    - 顔の位置補完(IOU)\n",
    "    - 並列でVideoWriter書き込み\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # プログラム開始時にLUTファイルをロード\n",
    "    lut = load_cube_file(\"DJI OSMO Action 5 Pro D-Log M to Rec.709 V1.cube\")\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # 別スレッドで書き出し\n",
    "    frame_queue = Queue(maxsize=10)\n",
    "    writer_thread = FrameWriter(out_writer, frame_queue)\n",
    "    writer_thread.start()\n",
    "\n",
    "    detect_interval = 2\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 顔トラックをIOUで補完\n",
    "    face_tracks = []\n",
    "    max_life = 4\n",
    "\n",
    "    # バッチ用\n",
    "    frame_batch = []       # リサイズ済みフレーム (推論用)\n",
    "    original_frames = []   # 元のフレーム (ぼかし用)\n",
    "    detect_flags = []      # このフレームで検出するか否か (True/False)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # (1) フレームをバッチに追加\n",
    "        resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "        frame_batch.append(resized_frame)\n",
    "        original_frames.append(frame)\n",
    "\n",
    "        # \"2フレームに1回\" で検出\n",
    "        detect_flags.append(frame_count == 1 or (frame_count % detect_interval == 0))\n",
    "\n",
    "        # (2) バッチがいっぱい or 最終フレーム\n",
    "        if len(frame_batch) == batch_size or frame_count == total_frames:\n",
    "            # ------ YOLO 推論 ------\n",
    "            # sub-batch: detect_flags が True のフレームだけまとめて推論する\n",
    "            sub_tensors = []\n",
    "            sub_indices = []\n",
    "            for i, (f, flag) in enumerate(zip(frame_batch, detect_flags)):\n",
    "                if flag:  # このフレームで検出実施\n",
    "                    tensor = torch.from_numpy(f).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "                    sub_tensors.append(tensor)\n",
    "                    sub_indices.append(i)\n",
    "\n",
    "            # sub_tensors をまとめて推論\n",
    "            if len(sub_tensors) > 0:\n",
    "                batch_tensor = torch.cat(sub_tensors, dim=0)\n",
    "                results = model.predict(\n",
    "                    batch_tensor, \n",
    "                    verbose=False,\n",
    "                    imgsz=(new_w, new_h),\n",
    "                    conf=0.25,\n",
    "                    iou=0.3,\n",
    "                    agnostic_nms=True\n",
    "                )\n",
    "            else:\n",
    "                results = []\n",
    "\n",
    "            # ------ 結果をフレームごとに整理 ------\n",
    "            # sub_indices と results[i] を対応付ける\n",
    "            #  => detection_results[フレームバッチ内index] = [(x1,y1,x2,y2), ...]\n",
    "            detection_results = [[] for _ in range(len(frame_batch))]\n",
    "            for r_i, r in enumerate(results):\n",
    "                i_batch_index = sub_indices[r_i]\n",
    "                new_faces = []\n",
    "                for box in r.boxes.xyxy:\n",
    "                    x1_r, y1_r, x2_r, y2_r = map(int, box)\n",
    "                    # オリジナル座標に戻す\n",
    "                    x1 = int(x1_r * width / new_w)\n",
    "                    y1 = int(y1_r * height / new_h)\n",
    "                    x2 = int(x2_r * width / new_w)\n",
    "                    y2 = int(y2_r * height / new_h)\n",
    "                    # 有効範囲チェック\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    new_faces.append((x1, y1, x2, y2))\n",
    "                detection_results[i_batch_index] = new_faces\n",
    "\n",
    "            # ------ 各フレームに対して IOU 補完 & ぼかし ------\n",
    "            # バッチ内フレームを順に処理\n",
    "            for i in range(len(frame_batch)):\n",
    "                # 新規検出があればトラック更新\n",
    "                new_dets = detection_results[i] if i < len(detection_results) else []\n",
    "                face_tracks = update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=max_life)\n",
    "\n",
    "                # 顔トラックをぼかし\n",
    "                for (fx1, fy1, fx2, fy2, _) in face_tracks:\n",
    "                    face_roi = original_frames[i][fy1:fy2, fx1:fx2]\n",
    "                    original_frames[i][fy1:fy2, fx1:fx2] = blur_face(face_roi)\n",
    "\n",
    "                # フレーム全体にLUT変換を適用\n",
    "                processed_frame = apply_3d_lut(original_frames[i], lut)\n",
    "\n",
    "                # 出力キューにフレームを詰める\n",
    "                frame_queue.put(processed_frame)\n",
    "\n",
    "            # 進捗ログ\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                remaining = (elapsed / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames}, Estimated remaining: {remaining:.1f} sec\")\n",
    "\n",
    "            # バッチをクリア\n",
    "            frame_batch = []\n",
    "            original_frames = []\n",
    "            detect_flags = []\n",
    "\n",
    "    # 終了処理\n",
    "    cap.release()\n",
    "\n",
    "    # フレーム書き出しスレッド終了\n",
    "    writer_thread.stop()\n",
    "    writer_thread.join()\n",
    "    out_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) 実行例\n",
    "# -------------------------------------------------------------\n",
    "process_video(\"input.mp4\", \"output.mp4\", batch_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Frame 100/615, Estimated remaining: 261.4 sec\n",
    "Frame 200/615, Estimated remaining: 207.8 sec\n",
    "Frame 300/615, Estimated remaining: 157.4 sec\n",
    "Frame 400/615, Estimated remaining: 107.6 sec\n",
    "Frame 500/615, Estimated remaining: 57.6 sec\n",
    "\n",
    "Total processing time: 299.4 seconds\n",
    "```\n",
    "\n",
    "メモ：色味がおかしく、遅い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結論、LUTはpython上で実行すべきではないかな"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 仕切り直し"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動画の回転を考慮する（22.1 seconds）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from queue import Queue\n",
    "import threading\n",
    "import ffmpeg  # ffmpeg-python を利用\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) 設定: デバイス・モデル・stride32リサイズ関数\n",
    "# -------------------------------------------------------------\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"\n",
    "    画像を YOLO のstride=32 の倍数 (height, width) にリサイズする\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width  = (width  // 32) * 32 + (32 if width  % 32 != 0 else 0)\n",
    "    if new_width == width and new_height == height:\n",
    "        return image, width, height\n",
    "    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized, new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"\n",
    "    顔部分を縮小してガウシアンぼかしをかけてから元サイズに戻す\n",
    "    \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) 顔の位置補完 (IOU管理)\n",
    "# -------------------------------------------------------------\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    2つのバウンディングボックス(boxA, boxB)に対するIoU(Intersection over Union)を計算\n",
    "    box = (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=4):\n",
    "    \"\"\"\n",
    "    前フレームまでの顔領域リスト(face_tracks)に、今フレームの検出結果(new_dets)を反映して更新する。\n",
    "      - face_tracks: [(x1, y1, x2, y2, life), ... ]\n",
    "      - new_dets:    [(x1, y1, x2, y2), ... ]\n",
    "    iou_thresh: IoU がこの値以上なら同じ顔とみなす\n",
    "    max_life:   ライフ上限(継続保持するフレーム数)\n",
    "    \"\"\"\n",
    "    updated_tracks = []\n",
    "    for (tx1, ty1, tx2, ty2, life) in face_tracks:\n",
    "        updated_tracks.append([tx1, ty1, tx2, ty2, life - 1])\n",
    "    for (nx1, ny1, nx2, ny2) in new_dets:\n",
    "        best_iou = 0\n",
    "        best_index = -1\n",
    "        for i, (tx1, ty1, tx2, ty2, life) in enumerate(updated_tracks):\n",
    "            iou_val = compute_iou((tx1, ty1, tx2, ty2), (nx1, ny1, nx2, ny2))\n",
    "            if iou_val > best_iou:\n",
    "                best_iou = iou_val\n",
    "                best_index = i\n",
    "        if best_iou >= iou_thresh and best_index >= 0:\n",
    "            updated_tracks[best_index][0] = nx1\n",
    "            updated_tracks[best_index][1] = ny1\n",
    "            updated_tracks[best_index][2] = nx2\n",
    "            updated_tracks[best_index][3] = ny2\n",
    "            updated_tracks[best_index][4] = max_life\n",
    "        else:\n",
    "            updated_tracks.append([nx1, ny1, nx2, ny2, max_life])\n",
    "    filtered_tracks = []\n",
    "    for t in updated_tracks:\n",
    "        if t[4] > 0:\n",
    "            filtered_tracks.append(t)\n",
    "    return filtered_tracks\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) 動画書き出しを並列化するクラス\n",
    "# -------------------------------------------------------------\n",
    "class FrameWriter(threading.Thread):\n",
    "    \"\"\"\n",
    "    別スレッドでフレームを書き込む。\n",
    "      - メインスレッドでフレームを queue に put する\n",
    "      - ここで queue.get() して VideoWriter.write() する\n",
    "    \"\"\"\n",
    "    def __init__(self, video_writer, frame_queue):\n",
    "        super().__init__()\n",
    "        self.video_writer = video_writer\n",
    "        self.frame_queue = frame_queue\n",
    "        self.stop_signal = False\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            if self.stop_signal and self.frame_queue.empty():\n",
    "                break\n",
    "            try:\n",
    "                frame = self.frame_queue.get(timeout=0.1)\n",
    "            except:\n",
    "                continue\n",
    "            self.video_writer.write(frame)\n",
    "            self.frame_queue.task_done()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) メイン処理: バッチ推論 + 2フレームに1回検出 + IOU補完 + 並列書き出し\n",
    "# -------------------------------------------------------------\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    # ffmpeg.probe で動画のメタデータを取得し、回転情報を抽出\n",
    "    metadata = ffmpeg.probe(input_path)\n",
    "    stream0 = metadata['streams'][0]\n",
    "    rotate_tag = stream0.get('tags', {}).get('rotate', None)\n",
    "    if rotate_tag is None and 'side_data_list' in stream0:\n",
    "        for side in stream0['side_data_list']:\n",
    "            if side.get('side_data_type') == 'Display Matrix' and 'rotation' in side:\n",
    "                rotate_tag = side['rotation']\n",
    "                break\n",
    "    if rotate_tag is None:\n",
    "        rotate_tag = '0'\n",
    "    rotation_angle = int(rotate_tag)\n",
    "    print(f\"動画の回転情報: {rotation_angle}°\")\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # 回転補正後の出力サイズを設定（90°または270°なら幅と高さを入れ替え）\n",
    "    if rotation_angle in [90, 270]:\n",
    "        output_size = (height, width)\n",
    "    else:\n",
    "        output_size = (width, height)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out_writer = cv2.VideoWriter(output_path, fourcc, fps, output_size)\n",
    "\n",
    "    frame_queue = Queue(maxsize=10)\n",
    "    writer_thread = FrameWriter(out_writer, frame_queue)\n",
    "    writer_thread.start()\n",
    "\n",
    "    detect_interval = 2\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    face_tracks = []\n",
    "    max_life = 4\n",
    "\n",
    "    frame_batch = []\n",
    "    original_frames = []\n",
    "    detect_flags = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # ----- 回転補正: メタデータの回転情報に基づいて各フレームを補正 -----\n",
    "        # 補正角度を正の値に変換\n",
    "        norm_angle = abs(rotation_angle)\n",
    "        if norm_angle == 90:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif norm_angle == 180:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
    "        elif norm_angle == 270:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "        frame_batch.append(resized_frame)\n",
    "        original_frames.append(frame)\n",
    "        detect_flags.append(frame_count == 1 or (frame_count % detect_interval == 0))\n",
    "\n",
    "        if len(frame_batch) == batch_size or frame_count == total_frames:\n",
    "            sub_tensors = []\n",
    "            sub_indices = []\n",
    "            for i, (f, flag) in enumerate(zip(frame_batch, detect_flags)):\n",
    "                if flag:\n",
    "                    tensor = torch.from_numpy(f).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "                    sub_tensors.append(tensor)\n",
    "                    sub_indices.append(i)\n",
    "            if len(sub_tensors) > 0:\n",
    "                batch_tensor = torch.cat(sub_tensors, dim=0)\n",
    "                results = model.predict(\n",
    "                    batch_tensor, \n",
    "                    verbose=False,\n",
    "                    imgsz=(new_w, new_h),\n",
    "                    conf=0.25,\n",
    "                    iou=0.3,\n",
    "                    agnostic_nms=True\n",
    "                )\n",
    "            else:\n",
    "                results = []\n",
    "\n",
    "            detection_results = [[] for _ in range(len(frame_batch))]\n",
    "            for r_i, r in enumerate(results):\n",
    "                i_batch_index = sub_indices[r_i]\n",
    "                new_faces = []\n",
    "                for box in r.boxes.xyxy:\n",
    "                    x1_r, y1_r, x2_r, y2_r = map(int, box)\n",
    "                    x1 = int(x1_r * width / new_w)\n",
    "                    y1 = int(y1_r * height / new_h)\n",
    "                    x2 = int(x2_r * width / new_w)\n",
    "                    y2 = int(y2_r * height / new_h)\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    new_faces.append((x1, y1, x2, y2))\n",
    "                detection_results[i_batch_index] = new_faces\n",
    "\n",
    "            for i in range(len(frame_batch)):\n",
    "                new_dets = detection_results[i] if i < len(detection_results) else []\n",
    "                face_tracks = update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=max_life)\n",
    "                for (fx1, fy1, fx2, fy2, _) in face_tracks:\n",
    "                    face_roi = original_frames[i][fy1:fy2, fx1:fx2]\n",
    "                    original_frames[i][fy1:fy2, fx1:fx2] = blur_face(face_roi)\n",
    "                frame_queue.put(original_frames[i])\n",
    "\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                remaining = (elapsed / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames}, Estimated remaining: {remaining:.1f} sec\")\n",
    "            frame_batch = []\n",
    "            original_frames = []\n",
    "            detect_flags = []\n",
    "\n",
    "    cap.release()\n",
    "    writer_thread.stop()\n",
    "    writer_thread.join()\n",
    "    out_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) 実行例\n",
    "# -------------------------------------------------------------\n",
    "process_video(\"input.mp4\", \"output.mp4\", batch_size=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "動画の回転情報: -180°\n",
    "Frame 100/644, Estimated remaining: 24.9 sec\n",
    "Frame 200/644, Estimated remaining: 18.3 sec\n",
    "Frame 300/644, Estimated remaining: 13.6 sec\n",
    "Frame 400/644, Estimated remaining: 9.3 sec\n",
    "Frame 500/644, Estimated remaining: 5.4 sec\n",
    "\n",
    "Total processing time: 22.1 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 音声データ、メタデータを移植する（21.5 seconds）\n",
    "音声、メタデータがついた。  \n",
    "回転のデータ（side_data_list）は移植されなかったので、  \n",
    "結果的に二重で回転しなかったが、これでよかったのか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "from queue import Queue\n",
    "import threading\n",
    "import ffmpeg  # ffmpeg-python を利用\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1) 設定: デバイス・モデル・stride32リサイズ関数\n",
    "# -------------------------------------------------------------\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model = YOLO(\"yolov11n-face.pt\").to(device)\n",
    "\n",
    "cv2.setNumThreads(cv2.getNumberOfCPUs())\n",
    "\n",
    "def resize_to_stride32(image):\n",
    "    \"\"\"\n",
    "    画像を YOLO のstride=32 の倍数 (height, width) にリサイズする\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    new_height = (height // 32) * 32 + (32 if height % 32 != 0 else 0)\n",
    "    new_width  = (width  // 32) * 32 + (32 if width  % 32 != 0 else 0)\n",
    "    if new_width == width and new_height == height:\n",
    "        return image, width, height\n",
    "    resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "    return resized, new_width, new_height\n",
    "\n",
    "def blur_face(image, ksize=(15, 15)):\n",
    "    \"\"\"\n",
    "    顔部分を縮小してガウシアンぼかしをかけてから元サイズに戻す\n",
    "    \"\"\"\n",
    "    if image.size == 0:\n",
    "        return image\n",
    "    small = cv2.resize(image, ksize, interpolation=cv2.INTER_LINEAR)\n",
    "    blurred = cv2.GaussianBlur(small, (5, 5), 0)\n",
    "    return cv2.resize(blurred, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2) 顔の位置補完 (IOU管理)\n",
    "# -------------------------------------------------------------\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    2つのバウンディングボックス(boxA, boxB)に対するIoU(Intersection over Union)を計算\n",
    "    box = (x1, y1, x2, y2)\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=4):\n",
    "    \"\"\"\n",
    "    前フレームまでの顔領域リスト(face_tracks)に、今フレームの検出結果(new_dets)を反映して更新する。\n",
    "      - face_tracks: [(x1, y1, x2, y2, life), ... ]\n",
    "      - new_dets:    [(x1, y1, x2, y2), ... ]\n",
    "    iou_thresh: IoU がこの値以上なら同じ顔とみなす\n",
    "    max_life:   ライフ上限(継続保持するフレーム数)\n",
    "    \"\"\"\n",
    "    updated_tracks = []\n",
    "    for (tx1, ty1, tx2, ty2, life) in face_tracks:\n",
    "        updated_tracks.append([tx1, ty1, tx2, ty2, life - 1])\n",
    "    for (nx1, ny1, nx2, ny2) in new_dets:\n",
    "        best_iou = 0\n",
    "        best_index = -1\n",
    "        for i, (tx1, ty1, tx2, ty2, life) in enumerate(updated_tracks):\n",
    "            iou_val = compute_iou((tx1, ty1, tx2, ty2), (nx1, ny1, nx2, ny2))\n",
    "            if iou_val > best_iou:\n",
    "                best_iou = iou_val\n",
    "                best_index = i\n",
    "        if best_iou >= iou_thresh and best_index >= 0:\n",
    "            updated_tracks[best_index][0] = nx1\n",
    "            updated_tracks[best_index][1] = ny1\n",
    "            updated_tracks[best_index][2] = nx2\n",
    "            updated_tracks[best_index][3] = ny2\n",
    "            updated_tracks[best_index][4] = max_life\n",
    "        else:\n",
    "            updated_tracks.append([nx1, ny1, nx2, ny2, max_life])\n",
    "    filtered_tracks = []\n",
    "    for t in updated_tracks:\n",
    "        if t[4] > 0:\n",
    "            filtered_tracks.append(t)\n",
    "    return filtered_tracks\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3) 動画書き出しを並列化するクラス\n",
    "# -------------------------------------------------------------\n",
    "class FrameWriter(threading.Thread):\n",
    "    \"\"\"\n",
    "    別スレッドでフレームを書き込む。\n",
    "      - メインスレッドでフレームを queue に put する\n",
    "      - ここで queue.get() して VideoWriter.write() する\n",
    "    \"\"\"\n",
    "    def __init__(self, video_writer, frame_queue):\n",
    "        super().__init__()\n",
    "        self.video_writer = video_writer\n",
    "        self.frame_queue = frame_queue\n",
    "        self.stop_signal = False\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            if self.stop_signal and self.frame_queue.empty():\n",
    "                break\n",
    "            try:\n",
    "                frame = self.frame_queue.get(timeout=0.1)\n",
    "            except:\n",
    "                continue\n",
    "            self.video_writer.write(frame)\n",
    "            self.frame_queue.task_done()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4) メイン処理: バッチ推論 + 2フレームに1回検出 + IOU補完 + 並列書き出し\n",
    "# -------------------------------------------------------------\n",
    "def process_video(input_path, output_path, batch_size=4):\n",
    "    # ffmpeg.probe で動画のメタデータを取得し、回転情報を抽出\n",
    "    metadata = ffmpeg.probe(input_path)\n",
    "    stream0 = metadata['streams'][0]\n",
    "    rotate_tag = stream0.get('tags', {}).get('rotate', None)\n",
    "    if rotate_tag is None and 'side_data_list' in stream0:\n",
    "        for side in stream0['side_data_list']:\n",
    "            if side.get('side_data_type') == 'Display Matrix' and 'rotation' in side:\n",
    "                rotate_tag = side['rotation']\n",
    "                break\n",
    "    if rotate_tag is None:\n",
    "        rotate_tag = '0'\n",
    "    rotation_angle = int(rotate_tag)\n",
    "    print(f\"動画の回転情報: {rotation_angle}°\")\n",
    "\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # 回転補正後の出力サイズを設定（90°または270°なら幅と高さを入れ替え）\n",
    "    if rotation_angle in [90, 270]:\n",
    "        output_size = (height, width)\n",
    "    else:\n",
    "        output_size = (width, height)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out_writer = cv2.VideoWriter(output_path, fourcc, fps, output_size)\n",
    "\n",
    "    frame_queue = Queue(maxsize=10)\n",
    "    writer_thread = FrameWriter(out_writer, frame_queue)\n",
    "    writer_thread.start()\n",
    "\n",
    "    detect_interval = 2\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    face_tracks = []\n",
    "    max_life = 4\n",
    "\n",
    "    frame_batch = []\n",
    "    original_frames = []\n",
    "    detect_flags = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # ----- 回転補正: メタデータの回転情報に基づいて各フレームを補正 -----\n",
    "        # 補正角度を正の値に変換\n",
    "        norm_angle = abs(rotation_angle)\n",
    "        if norm_angle == 90:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif norm_angle == 180:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
    "        elif norm_angle == 270:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        resized_frame, new_w, new_h = resize_to_stride32(frame)\n",
    "        frame_batch.append(resized_frame)\n",
    "        original_frames.append(frame)\n",
    "        detect_flags.append(frame_count == 1 or (frame_count % detect_interval == 0))\n",
    "\n",
    "        if len(frame_batch) == batch_size or frame_count == total_frames:\n",
    "            sub_tensors = []\n",
    "            sub_indices = []\n",
    "            for i, (f, flag) in enumerate(zip(frame_batch, detect_flags)):\n",
    "                if flag:\n",
    "                    tensor = torch.from_numpy(f).permute(2, 0, 1).unsqueeze(0).to(device).float() / 255.0\n",
    "                    sub_tensors.append(tensor)\n",
    "                    sub_indices.append(i)\n",
    "            if len(sub_tensors) > 0:\n",
    "                batch_tensor = torch.cat(sub_tensors, dim=0)\n",
    "                results = model.predict(\n",
    "                    batch_tensor, \n",
    "                    verbose=False,\n",
    "                    imgsz=(new_w, new_h),\n",
    "                    conf=0.25,\n",
    "                    iou=0.3,\n",
    "                    agnostic_nms=True\n",
    "                )\n",
    "            else:\n",
    "                results = []\n",
    "\n",
    "            detection_results = [[] for _ in range(len(frame_batch))]\n",
    "            for r_i, r in enumerate(results):\n",
    "                i_batch_index = sub_indices[r_i]\n",
    "                new_faces = []\n",
    "                for box in r.boxes.xyxy:\n",
    "                    x1_r, y1_r, x2_r, y2_r = map(int, box)\n",
    "                    x1 = int(x1_r * width / new_w)\n",
    "                    y1 = int(y1_r * height / new_h)\n",
    "                    x2 = int(x2_r * width / new_w)\n",
    "                    y2 = int(y2_r * height / new_h)\n",
    "                    if x2 <= x1 or y2 <= y1 or x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n",
    "                        continue\n",
    "                    new_faces.append((x1, y1, x2, y2))\n",
    "                detection_results[i_batch_index] = new_faces\n",
    "\n",
    "            for i in range(len(frame_batch)):\n",
    "                new_dets = detection_results[i] if i < len(detection_results) else []\n",
    "                face_tracks = update_face_tracks(face_tracks, new_dets, iou_thresh=0.5, max_life=max_life)\n",
    "                for (fx1, fy1, fx2, fy2, _) in face_tracks:\n",
    "                    face_roi = original_frames[i][fy1:fy2, fx1:fx2]\n",
    "                    original_frames[i][fy1:fy2, fx1:fx2] = blur_face(face_roi)\n",
    "                frame_queue.put(original_frames[i])\n",
    "\n",
    "            if frame_count % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                remaining = (elapsed / frame_count) * (total_frames - frame_count)\n",
    "                print(f\"Frame {frame_count}/{total_frames}, Estimated remaining: {remaining:.1f} sec\")\n",
    "            frame_batch = []\n",
    "            original_frames = []\n",
    "            detect_flags = []\n",
    "\n",
    "    cap.release()\n",
    "    writer_thread.stop()\n",
    "    writer_thread.join()\n",
    "    out_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTotal processing time: {total_time:.1f} seconds\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5) 実行例\n",
    "# -------------------------------------------------------------\n",
    "process_video(\"input.mp4\", \"output_video.mp4\", batch_size=4)\n",
    "\n",
    "# 音声、メタデータの合成\n",
    "video = ffmpeg.input('output_video.mp4').video  # 出力済み映像\n",
    "audio = ffmpeg.input('input.mp4').audio   # 元の音声（存在すれば）\n",
    "\n",
    "# ffmpeg の出力設定:\n",
    "# - map_metadata=1 で、2 番目の入力（input.mp4）のメタデータをコピー\n",
    "# - vcodec='copy', acodec='copy' で再エンコードせずコピーする\n",
    "\n",
    "ffmpeg.output(video, audio, 'output.mp4',\n",
    "              map_metadata=1,\n",
    "              vcodec='copy',\n",
    "              acodec='copy').run(overwrite_output=True, quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "動画の回転情報: -180°\n",
    "Frame 100/644, Estimated remaining: 23.0 sec\n",
    "Frame 200/644, Estimated remaining: 17.3 sec\n",
    "Frame 300/644, Estimated remaining: 13.0 sec\n",
    "Frame 400/644, Estimated remaining: 9.0 sec\n",
    "Frame 500/644, Estimated remaining: 5.2 sec\n",
    "\n",
    "Total processing time: 21.5 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# これでやりたいこと（できること）は一旦やった。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
